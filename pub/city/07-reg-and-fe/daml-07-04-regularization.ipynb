{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.03 Regularization\n",
    "\n",
    "There is another way to deal with too much variance in a model - regularization.\n",
    "Regularization is not a substitute to model selection we just saw\n",
    "but a complement to it.\n",
    "Where model selection finds a model with a good balance between bias and variance,\n",
    "regularization flattens out the variance within the same model.\n",
    "A model with less bias and more variance can then be selected and\n",
    "its variance flattened out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Difficult Turn](fe-turn.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>fe-turn.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will give a different take on our full road trip as before.\n",
    "Import the same objects.\n",
    "But instead of the Linear Regression model we import the\n",
    "Ridge, Lasso and Elastic Net Regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we saw on the left hand side is that the linear regression\n",
    "assumes that the **variables are independent of each other**,\n",
    "yet $t^{16}$ is very dependent on $t^2$.\n",
    "To reduce this dependence a few techniques exist,these build a **cost function** in which\n",
    "having many coefficients that are not zero introduces a lot of error.\n",
    "This is called **regularization**, for **parametric** techniques we have:\n",
    "\n",
    "#### Ridge (often called `L2`)\n",
    "\n",
    "$$\\min_{w_1, \\dots, w_{d - 1}} \\|Xw_j - y\\|_2^2 + \\alpha\\|w_j\\|_2^2$$\n",
    "\n",
    "#### Lasso (often called `L1`)\n",
    "\n",
    "$$\\min_{w_1, \\dots, w_{d - 1}} \\frac{1}{2\\cdot N_{samples}} \\|Xw_j - y\\|_2^2 + \\alpha\\|w_j\\|_1$$\n",
    "\n",
    "#### Elastic Net\n",
    "\n",
    "$$\n",
    "\\gamma\\texttt{Lasso} + (1 - \\gamma)\\texttt{Ridge}\n",
    "\\space = \\min_{w_1, \\dots, w_{d - 1}} \\frac{1}{2\\cdot N_{samples}} \\|Xw_j - y\\|_2^2\n",
    "\\space + \\gamma\\alpha\\|w_j\\|_1 + \\frac{1 - \\gamma}{2}\\alpha\\|w_j\\|_2^2\n",
    "$$\n",
    "\n",
    "The regularization increases the error of the model the higher the (absolute)\n",
    "values of the weights become.\n",
    "We prevent this way big values for weight and a model going astray.\n",
    "On the other hand a very heavy regularization leads to\n",
    "a model which outputs the mean of the data for every prediction.\n",
    "In order to understand the idea behind these equations we need to take a detour\n",
    "and see different ways of defining distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Measures\n",
    "\n",
    "Until now e have only dealt with *euclidean distances*\n",
    "but that is not the only distance measure available.\n",
    "In machine learning algorithms and in literature you will often find mention of *L-norms*.\n",
    "An *L-norm* (also called $L^p$ space) often written as `L1`, `L2`, $\\|\\cdot\\|_1$ or $\\|\\cdot\\|_2$\n",
    "is a notation of a specific measure of distance.\n",
    "The most important one being `L2` ($\\|\\cdot\\|_2$), the euclidean distance.\n",
    "The L-norm is defined, for a vector of $i$ components, as:\n",
    "\n",
    "$$\\|x\\|_k = \\left( \\sum_{i=0}^{N} \\lvert x_i \\rvert ^k \\right)^{\\frac{1}{k}}$$\n",
    "\n",
    "Therefore $\\|x\\|_2$ (or `L2`) is our well known\n",
    "\n",
    "$$\\|x\\|_2 = \\sqrt{\\sum_{i=0}^{N} x_i^2}$$\n",
    "\n",
    "The sum of absolutes, or $\\|x\\|_1$ (or `L1`) turns to be resistant against outliers,\n",
    "and is sometimes preferred as a distance measure\n",
    "\n",
    "$$\\|x\\|_1 = \\sum_{i=0}^{N} \\lvert x_i \\rvert$$\n",
    "\n",
    "`L1` is also often called Manhattan or city block distance due to the square shaped blocks in America.\n",
    "When one navigates in a city made out of blocks one first walks the entire distance\n",
    "across one coordinate and then all distance across the other coordinate - or any combination\n",
    "of streets in between but the final distance is the same.\n",
    "The `L1` distance is hence the sum of the absolute values in each coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "Ridge regularizes by summing the squares of all weights multiplied by an $\\alpha$\n",
    "user defined term.\n",
    "Lasso uses the sum of the absolute values as the regularization.\n",
    "The user defined $\\alpha$ is a hyperparameter which is a way to control\n",
    "how heavily a model is regularized.\n",
    "We normally set $\\alpha$ to a small value\n",
    "but there are cases where a big value of $\\alpha$ is useful.\n",
    "\n",
    "The Elastic Net is just a combination of both Ridge and Lasso.\n",
    "There is some weighing in the Elastic Net that accounts for the fact that\n",
    "Ridge values are squares whilst Lasso values are absolute values and hence\n",
    "smaller.\n",
    "The extra hyperparameter $\\gamma$ gives us control how much of\n",
    "Ridge and how much of Lasso regularization we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Techniques\n",
    "\n",
    "*Parametric techniques* is a name we use for techniques in which the number\n",
    "of model parameters is defined by the values of the *hyperparameters* used.\n",
    "In the case of the linear regression and its regularized variants\n",
    "the number of degrees in the polynomial features determines the number of weights.\n",
    "\n",
    "One can argue that until now we were not doing machine learning,\n",
    "since the linear regression without regularization has a closed form,\n",
    "i.e. there exist an algebraic direct solution for a plain linear regression.\n",
    "There is no algebraic solution for a regularized model,\n",
    "instead an optimizer is used to find the best solution.\n",
    "When one works on a hard or unknown problem it is often wise\n",
    "to add a small amount of regularization (small $\\alpha$)\n",
    "in order to make more likely for the problem to converge to\n",
    "a meaningful solution on a first try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the full road trip again and give this regularization a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 30*np.random.rand(256)\n",
    "spd = 13*np.sin(t/2) + 3.7*np.cos(t/2+7) + 3*t + 0.1*(t-10)**2 - 3*(t-3) + 7 + 2.3*np.random.randn(*t.shape)\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.plot(t, spd, 'o', color='crimson')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience tells me that for the problem at hand Lasso is the type\n",
    "of regularization that will best work on our problem of heavily related inputs.\n",
    "Ridge tends to reduce all weights similarly,\n",
    "whilst Lasso's effect often reduces several weights to zero\n",
    "and keeps reasonably high values for other weights.\n",
    "The rule of thumb is to:\n",
    "\n",
    "- If you know that you have the correct number of inputs into the problem,\n",
    "  e.g. you know from some theoretical background what kind of function fit\n",
    "  you are looking for, then Ridge should be the first choice.\n",
    "\n",
    "- On the other hand, if you believe that the majority of inputs into the problem\n",
    "  need to be culled then Lasso is the first choice.\n",
    "\n",
    "- For things in between ElasticNet provides full flexibility.\n",
    "\n",
    "For the full road trip problem the choice of regularization does not have\n",
    "a big impact, we encourage you to try out different regularization methods.\n",
    "Below we use a model that we well know to heavily overfit - too much model variance.\n",
    "We use a big value of $\\alpha$ to attempt to cull the extraneous model variance.\n",
    "We add `tol=0.01` as one possibility of model convergence: if the error between\n",
    "two optimizer iterations is smaller than $0.01$ then we consider that the model\n",
    "has reached a good solution.\n",
    "But since it is a problem we know to heavily overfit we also add\n",
    "`max_iter=300000` in order for it to not run too long.\n",
    "We may see a convergence warning if the model reaches that many optimizer iterations\n",
    "without reaching the desired tolerance but even then the model is likely\n",
    "to have reached a much better solution than without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=100), Lasso(alpha=10.0, tol=0.01, max_iter=300000))\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "model, model.named_steps.lasso.intercept_, model.named_steps.lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that we know that a polynomial of degree 100 heavily *overfits* the data,\n",
    "and, moreover, its parameters start to influence each other, we still managed to find a good fit.\n",
    "This is because, thanks to the regularization, the parameters that were heavily dependent\n",
    "on each other were forced to be very close to zero.\n",
    "Also note how the majority of the weights are tiny numbers very close to zero,\n",
    "only a few weights are effectively used during model prediction.\n",
    "\n",
    "This is good but likely this is still not the best solution we can achieve.\n",
    "Now we got another *hyperparameter* to tune: the *alpha* of the regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection - Again\n",
    "\n",
    "We did this before but now we have two hyperparameters to tune:\n",
    "polynomial *degree* and ridge *alpha*.\n",
    "The grid search uses a double underscore (`__`) to indicate a hyperparameter\n",
    "(argument to model constructor).\n",
    "In a pipeline the - all lowercase - name of the model, followed by the double\n",
    "underscore, followed by the hyperparameter values to try;\n",
    "performs the training and cross-validation across *all combinations* in the grid.\n",
    "\n",
    "since we will be training lots of models we reduce the number of\n",
    "`max_iter=` to a sensible value that will not make us wait hours for the cell to complete.\n",
    "Most of these models will *not* converge to a solution but the cross validation\n",
    "will just consider them to be bad models and discard them.\n",
    "In order to not fill our screen with warnings we ask Python\n",
    "to ignore convergence warnings from `sklearn` and matrix\n",
    "transformation warnings from the optimizer working below the hood.\n",
    "\n",
    "And since we are performing the model selection on the number of polynomial features,\n",
    "i.e. on the number of inputs into the model itself,\n",
    "the use of Lasso regularization makes less sense.\n",
    "Lasso would attempt to reduce several input to zero but the model\n",
    "selection would attempt to achieve the same.\n",
    "Instead we will use Ridge regularization for our model.\n",
    "It is likely that we can build in an even better model\n",
    "with the correct weighing of Lasso and Ridge regularization in an Elastic Net\n",
    "but we will stick to Ridge for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', category=LinAlgWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=5), Ridge(alpha=1.0, tol=0.1, max_iter=3000))\n",
    "grid = GridSearchCV(model,\n",
    "                    {'polynomialfeatures__degree': list(range(5, 21)),\n",
    "                     'ridge__alpha': [0.1, 0.5, 1, 2, 3, 5, 10, 20, 50, 100, 200, 300, 500]},\n",
    "                    cv=5)\n",
    "grid.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "grid.best_estimator_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a quite good estimator.\n",
    "Let's see how it plots over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = grid.best_estimator_.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home message - Generalization\n",
    "\n",
    "We managed to estimate a quite complex function.  But most importantly, we learned that we\n",
    "have an arsenal of machine learning automation in `sklearn`, cross-validation and grid-search\n",
    "are common to almost every machine learning problem, no matter the model used.\n",
    "\n",
    "Also, we saw a model that *underfits* - has high *bias* - and a model that *overfits* - has high *variance*.\n",
    "This is not to be confused with high variance in the data.\n",
    "Models with either too high bias or variance will **generalize** poorly.\n",
    "In other words, when we perform model selection we are searching for the model that best *generalizes*.\n",
    "And we can only say that a model generalizes well to new data if we can prove that\n",
    "the model does not underfit or overfit the data independently on how we test it.\n",
    "\n",
    "Note: Just because we got the model that best generalizes across a huge grid of hyperparameters\n",
    "and across a vast cross-validation, it does not mean we have the best model that we can get.\n",
    "We have the model that can best generalize under the assumptions we make when building it.\n",
    "For example, trying a specific ML technique is an assumption about the data.\n",
    "Comparing different ML techniques on the same data means comparing the best model\n",
    "(the one that best generalizes) built with each technique on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Common Pitfalls in Interpretation of Linear Models - SciKit Learn Documentation][1]\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html \"Common Pitfalls\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

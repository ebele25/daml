## daml-07-01-supervised-learning.ipynb 18:30-18:50

- give the examples of classification vs regression
- base rate fallacy and evaluation
- quickly about sklearn as a framework

## daml-07-02-regularization-lr.ipynb 18:50-19:30

- this notebook takes a while, it is fine to pass the break with it
- go through the idea of linear regression: line between points
- it can be solved analytically, and we trace the "best" line
- what is "best"?
- traditional way of linearising a dataset, logarithms
- trick with polynomial features
- coefficient of determination, R^2
- compute on several splits, cross-validation
- full road trip, underfitting (model bias) and overfitting (model variance)
- number of polynomial features is a hyperparameter
- regularization, counteracts overfitting
- generalization of a model is how it performs on unseen data

## pause 19:30-19:40

## daml-07-04-exercises-regr.ipynb 19:40-19:50

- just start it
- give the idea of single feature estimators, for the selection of features

## daml-07-03-fe-and-naive-bayes.ipynb 19:50-20:20

- transforming (categorical) strings to numbers
- one hot encoding
- textual data is even more complicated, TF-IDF
- parametric vs non-parametric (e.g. naive bayes) classifiers
- explain the confusion matrix, more useful than a score for multiclass
- the classifier is funny, be funny about it

## daml-07-06-exercises-clas.ipynb 20:20-20:30

- students will struggle to load the stop words file, they need to learn that
- open the sklearn docs and show them how to find the argument's name
- otherwise the exercise is a copy paste from the lecture

## daml-07-extra-least-squares.ipynb

- this is only for the students to read
- most grads would have had this at uni


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.08 Autoencoders\n",
    "\n",
    "We alluded a couple of times now to the idea that one can extract features\n",
    "automatically given that one has enough data.\n",
    "The reason this is possible is because complex models,\n",
    "such as NNs can build more conceptual representations of parts\n",
    "of the data and then combine those into the actual samples.\n",
    "This is a rather similar way we humans do:\n",
    "when thinking about an image of a car we think about the wheels,\n",
    "the fact that it has four of them, the windshield and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hamburger](ol-hamburger.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-hamburger.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a sandwich-like NN which is trained to replicate\n",
    "its input layer to its output layer.\n",
    "Let's make our sandwich a hamburger and try to describe the reasons\n",
    "for the architecture.\n",
    "\n",
    "The sandwich architecture means that the input and output layer\n",
    "have the same sizes, the bun of the hamburger to hold it.\n",
    "Going deeper we have some layers that are larger than the bun,\n",
    "as in the salad sticking out.\n",
    "Finally the middle layers, the hamburger itself,\n",
    "is smaller than the bun layers.\n",
    "The small middle forces the autoencoder to find an efficient representation\n",
    "of the data that passes through it,\n",
    "since a small layer cannot hold the full information in the dataset.\n",
    "\n",
    "The `sklearn` library is not particularly good to build NNs.\n",
    "But we will push `sklearn`'s capacities to the limit and use a `MLPRegressor`\n",
    "to build an autoencoder for parts of the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peculiar detail about training an autoencoder is that\n",
    "we take the data itself as its own labels during the training/fitting.\n",
    "We are pushing `sklearn`'s capacities to its limits\n",
    "so we will use the hyperbolic tangent as the activation function\n",
    "and will *not* scale the data so that the error in the higher values\n",
    "will push the network to work more in these areas.\n",
    "\n",
    "For a start we will fit our autoencoder on several images\n",
    "of a handwritten digit $3$.\n",
    "Note that the output is a matrix, not a single dimensional vector.\n",
    "This can be though of as if we are performing $64$ different,\n",
    "related, regressions as once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MLPRegressor(hidden_layer_sizes = (128, 8, 128), \n",
    "                   activation='tanh', learning_rate_init=0.0001, max_iter=1500, tol=0.001)\n",
    "X3 = digits.data[digits.target == 3]\n",
    "enc.fit(X3, X3)\n",
    "enc.n_layers_, enc.n_outputs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in `sklearn` there is an extra input layer the size of the data\n",
    "and an output layer the size of the labels.\n",
    "Both of which are $64$ digits, and hence neurons in size.\n",
    "This makes a sandwich architecture of $64$-$128$-$8$-$128$-$64$,\n",
    "in order of bun-salad-hamburger-salad-bun, i.e. $5$ layers.\n",
    "\n",
    "To evaluate our autoencoder we will predict values from a completely random start.\n",
    "We predict images full of white (random) noise and get outputs that\n",
    "we can evaluate by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = enc.predict(np.random.rand(3, 64))\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 7))\n",
    "for i in range(3):\n",
    "    axs.flat[i].imshow(y_hat[i].reshape(8, 8), cmap='binary')\n",
    "    axs.flat[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output suggests that it is the autoencoder that now understands\n",
    "how a digit $3$ looks.\n",
    "In other words we have a model that understand the inherent concepts\n",
    "of how a digit three looks.\n",
    "None of the images predicted exist in the dataset,\n",
    "it was the autoencoder itself that did draw them.\n",
    "\n",
    "We can perform something similar on the digit $6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc6 = MLPRegressor(hidden_layer_sizes=(128, 8, 128), \n",
    "                    activation='tanh', learning_rate_init=0.0001, max_iter=1500, tol=0.001)\n",
    "X6 = digits.data[digits.target == 6]\n",
    "enc6.fit(X6, X6)\n",
    "y_hat = enc6.predict(np.random.rand(3, 64))\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 7))\n",
    "for i in range(3):\n",
    "    axs.flat[i].imshow(y_hat[i].reshape(8, 8), cmap='binary')\n",
    "    axs.flat[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features inside the layer with only $8$ neurons compress the images\n",
    "in the entire dataset (of digit $6$ in this case) into an efficient representation.\n",
    "Such a representation is often non-linear and produces non-linear features.\n",
    "It is due to this efficient representation training that a deep NN\n",
    "(a NN with more than one hidden layer) can build features from the raw data.\n",
    "The bad side is that one needs a lot of data and training time\n",
    "in order to find such an efficient representation.\n",
    "\n",
    "In a real problem we would need a model that can identify more than a single class.\n",
    "Since we do not need machine learning to identify a single class in the first place.\n",
    "We will take three digits $2$, $5$ and $9$ and build an\n",
    "autoencoder that understand all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X259 = digits.data[(digits.target == 2) | (digits.target == 5) | (digits.target == 9)]\n",
    "y259 = digits.target[(digits.target == 2) | (digits.target == 5) | (digits.target == 9)]\n",
    "y259[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really the limit of `sklearn`.\n",
    "In a library meant for NNs one would be capable of defining different activation\n",
    "functions for different layers, which would considerably help the NN performance.\n",
    "Many libraries are also more efficient at calculating derivatives\n",
    "and can even perform the training inside a Graphics Processing Unit (GPU),\n",
    "which has optimized hardware for the matrix operations a NN requires.\n",
    "\n",
    "Still we can get some results from `sklearn` itself,\n",
    "just note that the following will take a really long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc259 = MLPRegressor(hidden_layer_sizes=(128, 8, 128), \n",
    "                      activation='tanh', learning_rate_init=0.0001, max_iter=15000, tol=0.0001)\n",
    "enc259.fit(X259, X259)\n",
    "y_hat = enc259.predict(X259[:6])\n",
    "fig, axs = plt.subplots(1, 6, figsize=(14, 7))\n",
    "for i in range(6):\n",
    "    axs.flat[i].imshow(y_hat[i].reshape(8, 8), cmap='binary')\n",
    "    axs.flat[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Photo](ol-photo.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-photo.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Metaphysics Time and Artificial Intelligence\n",
    "\n",
    "Metaphysics are often a debated subject but we will delve into it a bit\n",
    "to find some explanation as to why we often call machine learning\n",
    "to be artificial intelligence.\n",
    "And we are able to do it based on the last example.\n",
    "In the previous autoencoder we managed to build a model which\n",
    "*understands* three concepts: $2$, $5$ and $9$.\n",
    "And we will argue that because of that this autoencoder\n",
    "has an idea of the passing of time.\n",
    "\n",
    "One definition of artificial intelligence is whether a system\n",
    "operating against a human can fool the human into believing\n",
    "that the system is a human being.\n",
    "This is quick summary of the *Turing Test*.\n",
    "Unfortunately the Turing Test has been proven insufficient by internet chat bots,\n",
    "which are capable of passing the test - fooling a human - without\n",
    "making any use of machine learning.\n",
    "Since then we are struggling to better define artificial intelligence.\n",
    "\n",
    "A different way of describing whether a system is intelligent is to\n",
    "explore the concept of our own intelligence.\n",
    "We humans often argue that our intelligence is the capacity of laying\n",
    "our experience on a time line.\n",
    "Which means that intelligence requires the existence of time.\n",
    "As obvious as that may seem at first,\n",
    "the existence of time is not quite obvious once one looks deeper.\n",
    "In physics we define time to exist if and only if there exist occurrences\n",
    "that allow for this time to be counted.\n",
    "In a universe where there is nothing to count the passage of time, time would not exist.\n",
    "Note that the inner workings of your own mind are enough for you\n",
    "to be capable of counting time through these occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a similar argument to our autoencoder and to the definition\n",
    "of artificial intelligence.\n",
    "Since the existence of intelligence requires the existence of time,\n",
    "then the first step to achieve a system with artificial intelligence\n",
    "is to have a system with enough understanding to be capable of measuring\n",
    "the passage of time.\n",
    "To be capable of counting time one requires at least three different\n",
    "events: one baseline event, one event that changes from this baseline\n",
    "which is used as the measuring stick, and a third event that can be\n",
    "counted between the changes of the first two.\n",
    "\n",
    "Out autoencoder cannot know how many times it was presented an image\n",
    "of $2$ because it only exist whilst it is predicting.\n",
    "But the autoencoder is capable of understanding that there was a change\n",
    "from an image of $2$ to an image of $5$.\n",
    "It cannot count how many images of $2$ and how many images of $5$ it has\n",
    "seen because it cannot count time.\n",
    "But our autoencoder can notice a switch from $2$ to $5$, this gives it a measuring stick.\n",
    "Finally, our autoencoder can count whether a $9$ did appear between a change\n",
    "from $2$ to $5$ (or $5$ to $2$) or whether a $9$ did not appear.\n",
    "We therefore argue that the autoencoder has enough \"intelligence\"\n",
    "to be capable of counting the passage of time, the most basic \"intelligence\" as argued by metaphysics."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

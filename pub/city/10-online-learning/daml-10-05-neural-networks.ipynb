{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.04 Artificial Neural Networks\n",
    "\n",
    "If we place several perceptrons (neurons) together\n",
    "we can slice the search space with several lines.\n",
    "This is called an *Artificial Neural Network*\n",
    "(ANN, or just Neural Network, NN, for short).\n",
    "It is not trivial to train these neurons,\n",
    "after all we do not know what the output of most neurons should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first let's just use `sklearn`'s `MLPClassifier` (multi-layer-perceptron-classifier)\n",
    "as a model and see if we can make it work.\n",
    "We take all model evaluation common suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lieutenant Columbo](ol-columbo.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-columbo.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glass Dataset\n",
    "\n",
    "The forensics of glass composition can reveal the provenience of a piece of glass,\n",
    "yet different brands of glass are slightly different from each other.\n",
    "We will build a model which will classify glass based on its composition,\n",
    "and will use online learning so that we are ready to learn from new data at any time.\n",
    "If anyone asks us to be police detectives someday we now have a head start.\n",
    "\n",
    "This is a dataset at UCI machine learning repository, we need to build our `load_glass` function.\n",
    "The classes are numbered from $1$ in the dataset, and class $3$ is missing,\n",
    "we fix both issues during the data load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "def load_glass():\n",
    "    glass_dir = 'uci_glass'\n",
    "    data_dir = datasets.get_data_home()\n",
    "    data_path = os.path.join(data_dir, glass_dir, 'glass.data')\n",
    "    descr_path = os.path.join(data_dir, glass_dir, 'glass.names')\n",
    "    glass_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "    glass_descr = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.names'\n",
    "    os.makedirs(os.path.join(data_dir, glass_dir), exist_ok=True)\n",
    "    try:\n",
    "        with open(descr_path, 'r') as f:\n",
    "            descr = f.read()\n",
    "    except IOError:\n",
    "        print('Downloading file from', glass_descr, file=sys.stderr)\n",
    "        r = requests.get(glass_descr)\n",
    "        with open(descr_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        descr = r.text\n",
    "        r.close()\n",
    "    try:\n",
    "        data = pd.read_csv(data_path, delimiter=',', header=None).values\n",
    "    except IOError:\n",
    "        print('Downloading file from', glass_data, file=sys.stderr)\n",
    "        r = requests.get(glass_data)\n",
    "        with open(data_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        r.close()\n",
    "        data = pd.read_csv(data_path, delimiter=',', header=None).values\n",
    "    target = data[:, 10].astype(np.int).copy()\n",
    "    target[target > 3] -= 1  # fix non-existent classes\n",
    "    target -= 1              # fix class numbering\n",
    "    return Bunch(DESCR=descr,\n",
    "                 data=data[:, :10].copy(),\n",
    "                 feature_names=['ID', 'RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'],\n",
    "                 target=target,\n",
    "                 target_names=['windows_float_processed',\n",
    "                               'windows_non_float_processed',\n",
    "                               'vehicle_windows',\n",
    "                               'containers',\n",
    "                               'tableware',\n",
    "                               'headlamps'])\n",
    "\n",
    "\n",
    "glass = load_glass()\n",
    "print(glass.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The support of the classes is quite different.\n",
    "This isn't an easy problem to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(glass.target).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without knowing much about this model,\n",
    "let's try to use a neural network to classify this dataset.\n",
    "We know that the neural network is a good amount\n",
    "of interconnected perceptrons and that we perturb the perceptron weights\n",
    "based on errors in classification to achieve convergence.\n",
    "\n",
    "This is a real dataset,\n",
    "we may as well try to do things properly and take a test set out.\n",
    "This may prove a tad difficult due to the big differences in support.\n",
    "One could we may do is to select a good subset of classes and split train and test sets from these.\n",
    "But we will attempt to use the full dataset,\n",
    "we pass `stratify=` to `train_test_split` which will this way attempt\n",
    "to maintain class support the same across the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(glass.data, glass.target, test_size=0.2, stratify=glass.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know that `sklearn` provides us with a simple neural network class,\n",
    "no harm in trying it out.\n",
    "\n",
    "By now we can guess what `max_iter`, `alpha=` and `tol=` arguments are.\n",
    "The `solver=sgd` is stochastic gradient descent, where the default\n",
    "is an optimizer with many other heuristics called `adam`.\n",
    "The `hidden_layer_sizes=(20,)` means that we have a neural network\n",
    "of $10$ input perceptrons/neurons in a layer, the number of features in or dataset;\n",
    "$20$ perceptrons/neurons in a hidden layer that do the bulk of the processing;\n",
    "and $6$ perceptrons/neurons in an output layer, one for each class.\n",
    "The `activetion=` is the activation function of the perceptrons to which we will return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLPClassifier(activation='relu', hidden_layer_sizes=(20,),\n",
    "                    alpha=0.01, tol=0.001,\n",
    "                    max_iter=500, solver='sgd')\n",
    "param_dict = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'tol': [0.001, 0.01, 0.1],\n",
    "}\n",
    "grid = GridSearchCV(net, param_dict, cv=5)\n",
    "grid.fit(xtrain, ytrain)\n",
    "grid.best_estimator_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models may not converge but the cross-validation should root them out.\n",
    "And the best model should be a converged network.\n",
    "\n",
    "The score isn't bad but a classification report may give a better view\n",
    "of how a multilabel classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = grid.best_estimator_.predict(xtest)\n",
    "print(classification_report(ytest, y_hat, target_names=glass.target_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the support is low, we may have cases with zero precision or recall,\n",
    "which in turn will make a zero `F1` score due to a division by zero.\n",
    "That is the reason we added `zero_division=0` above to prevent zero division warnings.\n",
    "\n",
    "And a confusion matrix may give further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(ytest, y_hat)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "cax = ax.matshow(mat, cmap='summer')\n",
    "ticks = np.arange(0, len(glass.target_names))\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(glass.target_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(glass.target_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('true label')\n",
    "ax.set_xlabel('predicted label')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "for i in range(mat.shape[0]):\n",
    "    for j in range(mat.shape[1]):\n",
    "        ax.text(j, i, mat[i, j], ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we learned one of the most important details about neural networks:\n",
    "since weights are updated based on classification errors between expected and predicted\n",
    "classes, if we have classes with very little support the network will often fail at classifying them.\n",
    "\n",
    "As we get collect more data the effect of classes with smaller support diminishes.\n",
    "Neural Nets work in similar fashion to us, humans, you cannot show a person 2 paintings\n",
    "by Titian and then 200 painting not by Titian and expect her to be an expert at identifying Titian's work.\n",
    "ANNs work best the more data you have and the more diverse the data is.\n",
    "\n",
    "For the glass data we'll leave as an exercise trying out a Random Forest,\n",
    "that model is very good at digging out classes with small support\n",
    "(contrary to decision trees!).\n",
    "But for now let's understand more about ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[UCI - Glass Identification Dataset][1]\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/glass+identification \"UCI Glass\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.03 Forest Cover Type\n",
    "\n",
    "In `sklearn` we have `SDGClassifier` which will perform SDG\n",
    "to achieve online learning on top of linear SVMs, logistic regression, or a perceptron.\n",
    "The `SDGRegressor` performs a linear regression as online learning.\n",
    "Note that this means that we can only find solutions to problems\n",
    "that can be approximated linearly.\n",
    "For non-linear online learning we need neural networks (which we will see soon).\n",
    "\n",
    "We import the `SDGCLassifier` and the common preprocessors and validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest Cover Type Dataset\n",
    "\n",
    "For a change let's take on a dataset that is not present inside `sklearn`.\n",
    "The forest cover dataset are cartographic data about types of forests\n",
    "in the Roosevelt National Forest in Colorado.\n",
    "Since we are thinking of walking blindfolded around mountain ranges\n",
    "we may as well use a dataset of mountain forest coverage.\n",
    "\n",
    "First let's define a couple of details about the features of the dataset.\n",
    "It has several continuous features and then several categorical ones.\n",
    "The categorical features are already one-hot-encoded for us.\n",
    "The `wild=` areas are quite hidden in the dataset description,\n",
    "for easier access we added comments to the code below describing the features we will load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = [\n",
    "    'Elevation',\n",
    "    'Aspect',\n",
    "    'Slope',\n",
    "    'HHydro',\n",
    "    'VHydro',\n",
    "    'Road',\n",
    "    'Shade_9am',\n",
    "    'Shade_Noon',\n",
    "    'Shade_3pm',\n",
    "    'Fire',\n",
    "]\n",
    "categorical = [\n",
    "    'wild=1',  # Rawah Wilderness Area\n",
    "    'wild=2',  # Neota Wilderness Area\n",
    "    'wild=3',  # Comanche Peak Wilderness Area\n",
    "    'wild=4',  # Cache la Poudre Wilderness Area\n",
    "    'soil=1','soil=2','soil=3','soil=4','soil=5','soil=6','soil=7','soil=8','soil=9','soil=10',\n",
    "    'soil=11','soil=12','soil=13','soil=14','soil=15','soil=16','soil=17','soil=18','soil=19','soil=20',\n",
    "    'soil=21','soil=22','soil=23','soil=24','soil=25','soil=26','soil=27','soil=28','soil=29','soil=30',\n",
    "    'soil=31','soil=32','soil=33','soil=34','soil=35','soil=36','soil=37','soil=38','soil=39','soil=40',\n",
    "]\n",
    "columns = continuous + categorical + ['label']\n",
    "target_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine',\n",
    "                'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ponderosa Pine Forest Cover](ol-forest-ponderosa-pine.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-ponderosa-pine.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the features we can then classify an area of forest cover into\n",
    "one of the seven labels.\n",
    "The set has more then half a million rows of data, it a reasonably sized dataset.\n",
    "\n",
    "To keep with the spirit of what we have been doing until now we will\n",
    "write a function to actually retrieve the dataset.\n",
    "We will duplicate the `sklearn` convention for dataset loading and construct\n",
    "our `load_cover_type` function.\n",
    "The function not only allows for easy download of the dataset\n",
    "but also caches the downloaded data on the file system,\n",
    "so one does not need to download it the next time.\n",
    "A couple of things worth mentioning are that:\n",
    "the dataset is taken from the\n",
    "*University of California Irvine Machine Learning Repository*\n",
    "and is kept within their archives g-zipped.\n",
    "The decompression code is a tag obscure but it works on several corner cases.\n",
    "Also, the labels in the dataset start from $1$,\n",
    "we adjust the labels to start from $0$ during the dataset load.\n",
    "\n",
    "This code is pretty much what `sklearn` does every time we call one\n",
    "of its `load_*` procedures to get a dataset.\n",
    "The description of the forest cover dataset is very extensive,\n",
    "we print only the descriptions of the main features to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zlib\n",
    "import requests\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "def load_cover_type():\n",
    "    cov_dir = 'uci_cover_type'\n",
    "    data_dir = datasets.get_data_home()\n",
    "    data_path = os.path.join(data_dir, cov_dir, 'covtype.data')\n",
    "    descr_path = os.path.join(data_dir, cov_dir, 'covtype.info')\n",
    "    cov_data_gz = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "    cov_descr = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info'\n",
    "    os.makedirs(os.path.join(data_dir, cov_dir), exist_ok=True)\n",
    "    try:\n",
    "        with open(descr_path, 'r') as f:\n",
    "            descr = f.read()\n",
    "    except IOError:\n",
    "        print('Downloading file from', cov_descr, file=sys.stderr)\n",
    "        r = requests.get(cov_descr)\n",
    "        with open(descr_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        descr = r.text\n",
    "        r.close()\n",
    "    try:\n",
    "        data = pd.read_csv(data_path, delimiter=',', names=columns)\n",
    "    except IOError:\n",
    "        print('Downloading file from', cov_data_gz, file=sys.stderr)\n",
    "        r = requests.get(cov_data_gz)\n",
    "        cov_data = zlib.decompress(r.content, wbits=16+zlib.MAX_WBITS)  # obscure but works\n",
    "        cov_data = cov_data.decode('utf8')\n",
    "        with open(data_path, 'w') as f:\n",
    "            f.write(cov_data)\n",
    "        r.close()\n",
    "        data = pd.read_csv(data_path, delimiter=',', names=columns)\n",
    "    X = data[continuous + categorical].values\n",
    "    y = data['label'].values - 1\n",
    "    return Bunch(DESCR=descr,\n",
    "                 data=X,\n",
    "                 feature_names=columns[:-1],\n",
    "                 feature_continuous=continuous,\n",
    "                 feature_categorical=categorical,\n",
    "                 target=y,\n",
    "                 target_names=target_names)\n",
    "\n",
    "\n",
    "covtype = load_cover_type()\n",
    "print(covtype.DESCR[6923:8554])\n",
    "print()\n",
    "print(covtype.DESCR[12373:12713])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the dataset is always a good idea.\n",
    "One can see all three parts of the set:\n",
    "the continuous features, the one-hot-encoded categorical features,\n",
    "and the forest cover type labels.\n",
    "\n",
    "In the loading function we have also added a distinction between\n",
    "continuous and categorical features.\n",
    "This concept is often useful as one may need to scale continuous\n",
    "features but scaling one-hot-encoded features makes little sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(covtype.data, columns=covtype.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cottonwood Forest Cover](ol-forest-cottonwood.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-cottonwood.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half a million rows is a good dataset.\n",
    "It perhaps does not require online learning on most machines\n",
    "but on some it might.\n",
    "\n",
    "That said, for presentation purposes it may take too long\n",
    "to run out code on the full dataset.\n",
    "Instead we will take two forest types: Aspen (label $4$) and Douglas-fir (label $5$),\n",
    "and use only the part of the dataset with these labels.\n",
    "Note how we change the labels to be $0$ and $1$\n",
    "for a classification between only two forest types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = covtype.data\n",
    "y = covtype.target\n",
    "X = X[(y == 4) | (y == 5)].copy()\n",
    "y = y[(y == 4) | (y == 5)].copy()\n",
    "y[y == 4] = 0\n",
    "y[y == 5] = 1\n",
    "df = pd.DataFrame(X, columns=covtype.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Willow Forest Cover](ol-forest-willow.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-willow.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we can easily see that the continuous features\n",
    "have very distinct value ranges.\n",
    "And therefore will require scaling.\n",
    "\n",
    "We have the columns that are continuous in an attribute\n",
    "of the loaded dataset.\n",
    "If we now scale only those columns and place them back together\n",
    "with the categorical columns we have a dataset we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_cont = sc.fit_transform(df[covtype.feature_continuous].values)\n",
    "X_cat = df[covtype.feature_categorical].values\n",
    "X = np.c_[X_cont, X_cat]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a real dataset, we should treat it as a real problem.\n",
    "We take out a test set which we will not touch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train on the training set with cross-validation.\n",
    "The learning rate we will use will be constant with a value of `eta0=0.001`.\n",
    "Note also that SGD is an optimizer, not a model.\n",
    "The `loss=` argument chooses the model to optimize with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    PCA(n_components=10),\n",
    "    SGDClassifier(loss='log', penalty='l2', max_iter=500, alpha=0.01, tol=0.01,\n",
    "                  learning_rate='constant', eta0=0.001))\n",
    "param_grid = {\n",
    "    'sgdclassifier__alpha': [0.01, 0.1],\n",
    "    'sgdclassifier__tol': [0.01, 0.1],\n",
    "}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(xtrain, ytrain)\n",
    "grid.best_score_, grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Aspen Forest Cover](ol-forest-aspen.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-aspen.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally evaluate on the test set.\n",
    "And we have quite good scores for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Aspen', 'Douglas-fir']\n",
    "yfit = grid.best_estimator_.predict(xtest)\n",
    "print(classification_report(ytest, yfit, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Douglas-Fir Forest Cover](ol-forest-douglas-fir.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-douglas-fir.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey!  That wasn't online learning.\n",
    "\n",
    "Right, it was not.\n",
    "The full dataset may need online learning\n",
    "but out sample of two forest types only fit fine in memory.\n",
    "Yet we can make up the idea of a dataset that is too big\n",
    "by slitting into two sets of data and train SGD in an online fashion.\n",
    "We will *misuse* the `train_test_split` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov1, cov2, ycov1, ycov2 = train_test_split(X, y, test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also argue that we are facing a real world problem in which\n",
    "we only have a tiny bit of data ($10\\%$) right now.\n",
    "Whilst we wait for our rangers to collect more data for us we need to make\n",
    "do with what we already have and build a model.\n",
    "Later, when our forest rangers return with a lot more data,\n",
    "we will be able to use online learning to update the model.\n",
    "\n",
    "In `sklearn` there are two ways of using online learning.\n",
    "One is to use a method called `partial_fit`, instead of `fit`,\n",
    "which will update parameters instead of fitting completely new ones.\n",
    "Another way to enable online learning is to pass `warm_start=True`,\n",
    "this forces `fit` to always work like `partial_fit`.\n",
    "Both methods only work on models that support online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1, xtest1, ytrain1, ytest1 = train_test_split(cov1, ycov1, test_size=0.2)\n",
    "model = make_pipeline(\n",
    "    PCA(n_components=10),\n",
    "    SGDClassifier(loss='log', penalty='l2', max_iter=500, alpha=0.01, tol=0.01,\n",
    "                  learning_rate='constant', eta0=0.001, warm_start=True))\n",
    "model.fit(xtrain1, ytrain1)\n",
    "yfit = model.predict(xtest1)\n",
    "f1_score(ytest1, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know about a tiny bit of the data and we can, more-or-less, classify that.\n",
    "Note that since we know that we have rather similar scores for both classes\n",
    "on this dataset we are fine using a single score here.\n",
    "\n",
    "But if we try to classify the data we do not know about we may run into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain2, xtest2, ytrain2, ytest2 = train_test_split(cov2, ycov2, test_size=0.2)\n",
    "yfit = model.predict(xtest2)\n",
    "f1_score(ytest2, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model without the extra data provided later by the rangers works quite reasonably.\n",
    "\n",
    "But we can train with the extra data and see if things improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain2, ytrain2)\n",
    "yfit = model.predict(xtest2)\n",
    "f1_score(ytest2, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Krummholz Forest Cover](ol-forest-krummholz.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-krummholz.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Non-GD Optimisation\n",
    "\n",
    "We saw SGD and said that it is the most often used optimization technique.\n",
    "But what are the others?\n",
    "One technique is **simulated annealing** which works by slow cooling.\n",
    "The simulated annealing technique tries random neighbors at each iteration and keeps\n",
    "track of of the point with the lowest value of the error/cost function.\n",
    "The search space for a new neighbor (i.e. the maximum distance form the lowest point\n",
    "found until now) reduces at each iteration.\n",
    "This is similar to SGD with a decreasing learning rate.\n",
    "\n",
    "But there are more techniques.\n",
    "Notably **swarm intelligence** provides us with several optimization algorithms:\n",
    "\n",
    "- particle swarm\n",
    "- bat swarm\n",
    "- cuckoo search\n",
    "\n",
    "And **genetic algorithms** also work reasonably in an online learning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[UCI - Forest Cover Type Dataset][1]\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/Covertype \"UCI Forest Cover Type\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

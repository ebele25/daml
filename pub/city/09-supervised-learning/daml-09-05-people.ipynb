{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09.04 People\n",
    "\n",
    "SVMs are good for image recognition because of the high dimensional space,\n",
    "i.e. one dimension per pixel in the image.\n",
    "We will attempt to classify a dataset of faces.\n",
    "A face is a non-linear problem.\n",
    "For example we can tell that these two characters are related\n",
    "because the same actor did play them.\n",
    "But the only relation is the face,\n",
    "and perhaps the overly muscular body,\n",
    "the characters themselves have very little in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Characters](sl-stallone.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>sl-stallone.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the usual things and the SVM classifier.\n",
    "Also we import the loader for the Olivetti faces.\n",
    "The Olivetti faces is a set of $400$ images of faces, $10$ faces per person.\n",
    "It is a very clean dataset: the faces are well centered and the *support*\n",
    "of each class is the same across all classes.\n",
    "And since we are working with images we import PCA for noise reductions\n",
    "and model selection tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "ofaces = fetch_olivetti_faces()\n",
    "print(ofaces.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also look at the images to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(16, 16))\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(ofaces.images[i*2], cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can certainly see what was the glasses trend in the early $1990$s!\n",
    "\n",
    "This dataset is very ordered, each person's faces (class) are next to each other.\n",
    "If we will want to do cross validation we will need to shuffle it first,\n",
    "that is what `KFold` allows us to do.\n",
    "One peculiar thing about the `GridSearchCV` we will use is the kernel setting.\n",
    "Instead of a plain dictionary we provide a list\n",
    "and within the list we have the dictionaries for each kernel for the SVM.\n",
    "Here we try different kernels, and each requires a different set of hyperparameters.\n",
    "The `gamma` hyperparameter is the letter $\\eta$ in our `rbf` kernel definition,\n",
    "which letter is used in which notation often changes, beware!\n",
    "\n",
    "Remember that a double underscore is a bridge between the\n",
    "part of the pipeline and the hyperparameter name to set.\n",
    "If one looks inside the `sklearn` objects the underscores naming convention\n",
    "is accomplished by clever processing inside the `set_params` method,\n",
    "a method that exists in each model object in `sklearn`.\n",
    "The double underscore means to invoke `set_params` on the attribute named\n",
    "before the double underscore,\n",
    "and it can be repeated many times when dealing with deep pipelines.\n",
    "\n",
    "The following will take a while to complete when run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PCA(n_components=128, svd_solver='randomized'), SVC())\n",
    "strategy = KFold(n_splits=5, shuffle=True)\n",
    "param_grid = [\n",
    "    {'svc__kernel': ['linear'], 'svc__C': [1, 10, 100, 1000, 10000]},\n",
    "    {'svc__kernel': ['rbf'], 'svc__C': [1, 10, 1000, 10000], 'svc__gamma': [0.001, 0.1, 1.0, 10.0]}\n",
    "]\n",
    "grid = GridSearchCV(model, param_grid, cv=strategy)\n",
    "grid.fit(ofaces.data, ofaces.target)\n",
    "grid.best_score_, grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a pretty good score.\n",
    "SVMs are a good method for classifying images.\n",
    "And we also see what hyperparameters were used, throughout the best pipeline.\n",
    "\n",
    "The $C$ hyperparameter of an SVM is its tolerance to data points inside the support vectors.\n",
    "A smaller $C$ results in a model with better generalization\n",
    "(better at classifying unknown data) because it is less sensitive to noise in the data.\n",
    "Yet, a too small $C$ will undefit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we cannot really know the actual generalization score for these hyperparameters.\n",
    "This is because we tuned the hyperparameters against all data.\n",
    "This is hyperparameter overfit is a problem we will tackle next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass SVMs\n",
    "\n",
    "But before going forward, have you noticed something strange?\n",
    "I just told you that an SVM draws a (hyper)plane between two classes but we just\n",
    "classified $40$ different classes.\n",
    "What magic is happening here?\n",
    "\n",
    "In reality we have trained $40$ different SVMs, one for each class.\n",
    "An SVM is a **binary classifier**, i.e. it can only classify between two classes,\n",
    "often called the *negative class* and the *positive class*.\n",
    "What the Support Vector Classifier (`SVC`) performs is a technique called *One vs All*\n",
    "(OVA, or OVR - *One vs Rest*), in which it trains an SVM with the class\n",
    "as the positive class and all other data points as the negative class.\n",
    "The final prediction it the class that is further away from its own decision line,\n",
    "i.e. from all classifiers that consider a new data point as its positive class\n",
    "the `SVC` selects the SVM which is more confident about its result.\n",
    "\n",
    "*One vs One* (OVO) is another technique to make a binary classifier work on several classes.\n",
    "Whilst OVA trains a classifier for each class OVA trains a classifier for each pair of classes.\n",
    "The prediction is then done by majority voting across the SVMs.\n",
    "\n",
    "OVO trains more classifiers but OVA trains all classifiers on the full dataset.\n",
    "For example, above we would have trained $780$ different SVMs,\n",
    "one for each pair of classes $(combination(40, 2) = 780)$.\n",
    "For SVMs OVO is faster when there is a lot of data because an SVM takes much\n",
    "longer to train when the amount of data increases,\n",
    "i.e. training more small SVMs is faster than a few big SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OVO vs OVA](sl-ovo-ova.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>sl-ovo-ova.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we used a Logistic Regression to classify four different moons,\n",
    "behind the scenes `sklearn` has also used a One vs All trick.\n",
    "A Logistic Regression is a binary classifier after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [SciKit Learn - Face Decomposition][1]\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

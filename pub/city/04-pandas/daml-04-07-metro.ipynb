{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.06 Metro Traffic\n",
    "\n",
    "Let's look at an example where we perform analysis of a time series.\n",
    "We will check out some new techniques for plotting and aggregating along the way.\n",
    "We start with the common suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Metro Minneapolis](pd-metro.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>pd-metro.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The [Metro Transit][metro] in Minneapolis has a branch that runs between\n",
    "the two main cities in Minnesota: Minneapolis and Saint Paul.\n",
    "It forms the main Metropolitan area of Minnesota.\n",
    "The branch crosses the Mississippi river on its way.\n",
    "We will look at traffic data on the ATR $301$ station,\n",
    "more commonly known as Victoria Street Station.\n",
    "\n",
    "The dataset has been donated to, and can be downloaded from,\n",
    "the [Irvine Machine Learning Repository][dataset].\n",
    "But it has some duplicates, which I have culled before\n",
    "building the comma separated value (CSV) file we import below.\n",
    "\n",
    "A full transit dataset can always be downloaded from\n",
    "the [Minnesota Department of Transport][mndot]\n",
    "but out dataset has more information that has been added\n",
    "to the plain traffic data.\n",
    "\n",
    "[metro]: https://www.openstreetmap.org/#map=13/44.9518/-93.1963\n",
    "[dataset]: https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume\n",
    "[mndot]: https://www.dot.state.mn.us/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv` procedure in `pandas` is the de facto standard\n",
    "for data imports in PyData.\n",
    "NumPy provides the `loadtxt` procedure but `read_csv` can\n",
    "process missing data and many more flavors of data formats.\n",
    "Notably, CSV is a badly standardized format,\n",
    "and some clever heuristics are needed to parse some files.\n",
    "Moreover, `pandas` can parse the dates in the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pd-metro-traffic.csv', index_col='date_time', parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have hourly data on the passenger traffic on the westbound trains:\n",
    "from Saint Paul to Minneapolis.\n",
    "We also have a considerable amount of weather data.\n",
    "\n",
    "For a start let's see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traffic looks alright, three thousand people per hour is a reasonable number.\n",
    "The rain and snow data would be quite a lot of work to deal with so we will ignore those.\n",
    "And the temperature seems alright but a tad off in value.\n",
    "The temperature is in Kelvin but we will convert it to Celsius because\n",
    "it is easier to think about temperature in that scale.\n",
    "We also rename the index to a shorter name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic = df[['temp', 'traffic_volume']].copy()\n",
    "df_traffic.index.name = 'date'\n",
    "df_traffic.columns = ['temp', 'traffic']\n",
    "df_traffic['temp'] = df_traffic['temp'] - 273\n",
    "df_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is aggregated by the hour, something that will be important to keep in mind.\n",
    "\n",
    "We should plot it to get a better understanding.\n",
    "We will need `matplotlib` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` library wraps over `matplotlib` with its `plot` procedure,\n",
    "Most arguments are passed directly into `matplotlib` although\n",
    "there are several exceptions for that behavior.\n",
    "Whether to use `pandas`, `matplotlib` or a combination of both for plotting\n",
    "is a personal preference.\n",
    "Here we will use a combination of both to get some understanding on how they work together.\n",
    "\n",
    "We have two very distinct pieces of data to plot: traffic volume and temperature.\n",
    "One can build two separate vertical axes, one on the left another on the right\n",
    "with `maplotlib`'s `twinx` (there is also `twiny` but far less common).\n",
    "We then use one of the axes to plot the traffic and another to plot the temperature.\n",
    "We pass the axis we want to use in the `ax=` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axl = plt.subplots(figsize=(20, 9))\n",
    "axr = axl.twinx()\n",
    "df_traffic['traffic'].plot(alpha=0.6, ax=axl, style='.', color='limegreen')\n",
    "df_traffic['temp'].plot(alpha=0.6, ax=axr, style='.', color='deeppink')\n",
    "axl.set_ylabel('total traffic')\n",
    "axr.set_ylim(-50, 50)\n",
    "axr.set_ylabel('temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good representation of a real dataset,\n",
    "a good deal of missing data can be seen.\n",
    "The temperature changes regularly with the year\n",
    "but we can tell little about the traffic volume.\n",
    "\n",
    "The data is also too granular.\n",
    "If we aggregate by week we should see more.\n",
    "The `resample` procedure will allow us to aggregate on subsets of the time,\n",
    "here the `W` means week (a period definition in `pandas`).\n",
    "Before we used a scatter because we had $40$ thousand points,\n",
    "now we should be able to use lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axl = plt.subplots(figsize=(20, 9))\n",
    "axr = axl.twinx()\n",
    "weekly = df_traffic.resample('W').mean()\n",
    "weekly['traffic'].plot(ax=axl, color='limegreen')\n",
    "weekly['temp'].plot(ax=axr, color='deeppink')\n",
    "axl.set_ylabel('total traffic')\n",
    "axr.set_ylim(-50, 50)\n",
    "axr.set_ylabel('temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see a pattern here but it is still very rigged.\n",
    "\n",
    "Another aggregation technique we can use is a rolling window.\n",
    "A rolling window will aggregate over each period in consideration\n",
    "with a number of periods close to it.\n",
    "Here we will take days (using `resample`) and make a rolling window over $30$ days.\n",
    "Each day will have the mean traffic of itself and $29$ closest days.\n",
    "The `center=` argument asks for days both in the past and in the future\n",
    "when aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axl = plt.subplots(figsize=(20, 9))\n",
    "axr = axl.twinx()\n",
    "daily = df_traffic.resample('D').mean()\n",
    "daily = daily.rolling(30, center=True).mean()\n",
    "daily['traffic'].plot(ax=axl, color='limegreen')\n",
    "daily['temp'].plot(ax=axr, color='deeppink')\n",
    "axl.set_ylabel('total traffic')\n",
    "axr.set_ylim(-50, 50)\n",
    "axr.set_ylabel('temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good enough.  The rolling window flattens the noise.\n",
    "The only thing we could improve towards a presentation of the graph is the number of points.\n",
    "\n",
    "Above every day is still a single point on the graph,\n",
    "we could smoothen it by making a window over $5$ weeks instead of $30$ days.\n",
    "For an even smoother curve we could do a rolling window over $10$ weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axl = plt.subplots(figsize=(20, 9))\n",
    "axr = axl.twinx()\n",
    "weekly = df_traffic.resample('W').mean()\n",
    "weekly = weekly.rolling(5, center=True).mean()\n",
    "weekly['traffic'].plot(ax=axl, color='limegreen')\n",
    "weekly['temp'].plot(ax=axr, color='deeppink')\n",
    "axl.set_ylabel('mean weekly count')\n",
    "axr.set_ylim(-50, 50)\n",
    "axr.set_ylabel('temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial graph we could only see a mesh of points.\n",
    "Here we can see some patterns:\n",
    "The usage of the metro decreases during the holiday period,\n",
    "and during a short period in the summer.\n",
    "\n",
    "What we have been doing with the resampling\n",
    "was a from of grouping and then aggregating.\n",
    "We did the grouping of times close to each other\n",
    "but grouping of things away from each other is also possible and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "\n",
    "On a time series *grouping by* can produce completely different time frames,\n",
    "known as dicing and slicing the frame.\n",
    "On dates and times this division is quite evident: each day is formed of hours,\n",
    "each hour of minutes and so on.\n",
    "Let's first try to get a series of time stamps\n",
    "from our data and see how we can divide it into divisions such as weeks, days or hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df_traffic.index.to_series()\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know a year has $365$ or $366$ days,\n",
    "which makes it $8760$ or $8784$ hours during a year.\n",
    "\n",
    "Knowing this we can check how much missing data we have each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.groupby(series.dt.year).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a single complete year.\n",
    "Let us look at $2015$ and see how many days we actually have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2015 = series['2015']\n",
    "s2015.groupby(s2015.dt.year).count() / 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than $150$ days is not much to go about.\n",
    "Thankfully the other years are better.\n",
    "\n",
    "Now, one common pitfall to evaluate missing data in time series is to use the `info`\n",
    "procedure or the `isnull` procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `info` and `isnull` will tell us that there are no nulls.\n",
    "That is correct, there are no nulls.\n",
    "But a time series defines an initial time and a final time,\n",
    "and any points missing in the middle or not appearing at all is missing data.\n",
    "This also means that dropping missing data in a time series\n",
    "is a rather meaningless operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Patterns\n",
    "\n",
    "Knowing about grouping by we can slice the data in more ways now.\n",
    "Remember that several of the properties of the `dt` object are directly available from the index.\n",
    "For example, grouping by time (we only have hours) we can get the mean hourly traffic.\n",
    "Note the hourly ticks we setup in order to make a readable graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "by_time = df_traffic['traffic'].groupby(df_traffic.index.time).mean()\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.plot(xticks=hourly_ticks, ax=ax, color='forestgreen');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, yeah, those are people riding to work and riding back home.\n",
    "\n",
    "Grouping by the day of the week we get the amount of traffic across weekdays.\n",
    "Again, we set ticks to get a nice graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "by_weekday = df_traffic['traffic'].groupby(df_traffic.index.dayofweek).mean()\n",
    "weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "by_weekday.plot(xticks=np.arange(len(weekdays)), ax=ax, color='forestgreen')\n",
    "ax.set_xticklabels(weekdays);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekend is very different, as we'd expect.\n",
    "We can also check how the hourly distribution is on the weekend.\n",
    "We can build a multi-index by *grouping by* over two elements.\n",
    "\n",
    "Remember that the `where` procedure from NumPy works similar to an if-else loop.\n",
    "Also, `weekday` is an alias for `dayoftheweek`, as well as `day_name`.\n",
    "`pandas` often has aliases for the same attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend = np.where(df_traffic.index.weekday < 5, 'Weekday', 'Weekend')\n",
    "by_time = df_traffic['traffic'].groupby([weekend, df_traffic.index.time]).mean()\n",
    "by_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in a series with a two-level index,\n",
    "weekday of weekend and the hours.\n",
    "Now one can select from the first index and plot each piece of data side to side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.loc['Weekday'].plot(ax=ax, label='Weekdays', style='-', color='forestgreen')\n",
    "by_time.loc['Weekend'].plot(ax=ax, label='Weekends', style=':', color='darkorange')\n",
    "ax.legend()\n",
    "ax.set_xticks(hourly_ticks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekday behavior of riding to work and riding home is even more pronounced.\n",
    "But during the weekend we see a very different pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Extras\n",
    "\n",
    "[Group by - pandas user guide][1]\n",
    "\n",
    "[1]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

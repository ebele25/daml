{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08.01 Principal Component Analysis\n",
    "\n",
    "PCA is an unsupervised algorithm which reduces the feature space.\n",
    "The algorithm builds a projection of the feature space into a space\n",
    "with a smaller number of dimensions.\n",
    "The new feature space is not necessarily meaningful in terms of the original features,\n",
    "because the projection plane (or hyperplane) may cross the features diagonally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the idea of projections.\n",
    "Data points are no more than multidimensional arrays,\n",
    "and in Linear Algebra multiplication between arrays of\n",
    "different dimensions performs a projection between the dimensions.\n",
    "We import a handful of things and build a simple example of a function\n",
    "in $3$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have two classes: red and blue.\n",
    "These data will reside in three dimensions,\n",
    "and we could call the class itself a fourth dimension.\n",
    "The classes have no meaning in this case but they\n",
    "allow us to visualize which side of the data we are looking at.\n",
    "\n",
    "The *meshgrid* produces all combinations between $x$ and $y$ features,\n",
    "and we build a almost-dependent $z$ feature.\n",
    "The $z$ feature will be a function of $x$ and $y$ but\n",
    "with some noise added in.\n",
    "We have.\n",
    "\n",
    "$$\n",
    "z = f(x, y) \\approx x^2 + y\n",
    "$$\n",
    "\n",
    "And for the classes, we define them in terms of $x$ and $z$\n",
    "\n",
    "$$\n",
    "red = z > 20, x < 10 \\approx \\sqrt{20 - y} < x < 10\n",
    "$$\n",
    "\n",
    "The full set look as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 64\n",
    "x = np.linspace(0, 10, points) + 7*np.random.rand(points)\n",
    "y = np.linspace(0, 10, points) + 3*np.random.rand(points)\n",
    "gx, gy = np.meshgrid(x, y)\n",
    "z = 2*gx + gy + 15*np.random.rand(points, points)\n",
    "red = (z > 20) & (x < 10)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(gx[red], gy[red], z[red], c='crimson', alpha=0.7)\n",
    "ax.scatter(gx[~red], gy[~red], z[~red], c='steelblue', alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of the arrays is three dimensional\n",
    "but a s we saw before we can *melt* the three dimensional\n",
    "representation into a two dimensional one.\n",
    "We build a matrix (a two dimensional array or a data frame)\n",
    "representing each feature as a column.\n",
    "This is the most common data representation we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': gx.reshape(-1), 'y': gy.reshape(-1), 'z': z.reshape(-1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our data as a matrix we can do linear algebra on top of it.\n",
    "Matrix multiplication is only possible if the left operand has the same number\n",
    "of columns as the right operand has of rows.\n",
    "\n",
    "$$\n",
    "X_{n \\times m} T_{p \\times q}\n",
    "$$\n",
    "\n",
    "Is only possible if $m = p$.\n",
    "In other words $A_{2 \\times 3}$ can multiply $B_{3 \\times 5}$\n",
    "but $B_{3 \\times 5}$ cannot multiply $A_{2 \\times 3}$.\n",
    "Yes, you did read that right, matrix multiplication is not commutative.\n",
    "$A_{2 \\times 3}$ must be on the left hand side of the multiplication\n",
    "and $B_{3 \\times 5}$ must be on the right hand side.\n",
    "And the result of the multiplication will have\n",
    "$n$ rows and $q$ columns.\n",
    "\n",
    "$$\n",
    "X_{n \\times m} T_{p \\times q} = M_{n \\times q}\n",
    "$$\n",
    "\n",
    "Our piece of data is a matrix which we will call $X_{4096 \\times 3}$.\n",
    "We can (left) multiply a matrix $T_{n \\times 4096}$ by our matrix,\n",
    "or we can (right) multiply our matrix by a matrix $T_{3 \\times m}$.\n",
    "The terms left multiplication and right multiplication are from linear\n",
    "algebra and mean the use of the other matrix by which we multiply\n",
    "on the left or right of the matrix we want to multiply.\n",
    "\n",
    "Since $T_{3 \\times m}$ is a lot easier to build than $T_{n \\times 4096}$,\n",
    "we will build the former and left multiply our data by it.\n",
    "For simplicity we will use a square matrix $T_{3 \\times 3}$.\n",
    "We use the letter $T$ for this matrix to mean *transform*,\n",
    "and we write `TR` in the code.\n",
    "Note that `@` is Python's own matrix multiplication operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = np.array([\n",
    "    [1, 2, -1],\n",
    "    [0, 1,  3],\n",
    "    [2, 3,  1],\n",
    "])\n",
    "M = df.values @ TR\n",
    "mx, my, mz = M[:, 0], M[:, 1], M[:, 2]\n",
    "\n",
    "mred = red.reshape(-1)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(mx[mred], my[mred], mz[mred], c='crimson', alpha=0.7)\n",
    "ax.scatter(mx[~mred], my[~mred], mz[~mred], c='steelblue', alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we multiplied across the features of our $X_{4096 \\times 3}$\n",
    "we kept the number of samples ($4096$) and since the matrix\n",
    "$T_{3 \\times 3}$ is square we kept the number of features (dimensions).\n",
    "The end results is that the values in side $T_{3 \\times 3}$ move,\n",
    "rotate or stretch our data;\n",
    "but the end result is still $4096$ points in three dimensional space.\n",
    "\n",
    "The multiplication we are interested in though is to reduce the number\n",
    "of the dimensions.\n",
    "Since our $X_{4096 \\times 3}$ has $3$ columns then our $MT{3 \\times m}$\n",
    "must have $3$ rows in order for the multiplication to be possible (defined).\n",
    "But the number of columns of $T_{3 \\times m}$ is not required to be $3$.\n",
    "\n",
    "If we right multiply our data, $X_{4096 \\times 3}$ by a matrix\n",
    "$T_{3 \\times 2}$ the result will be some matrix $M_{4096 \\times 2}$.\n",
    "This $M_{4096 \\times 2}$ is a representation of $4096$ points in\n",
    "$2$ dimensions, hence we projected our our data points\n",
    "from $3$ dimensions into $2$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = np.array([\n",
    "    [1, -1],\n",
    "    [1,  2],\n",
    "    [3, -1],\n",
    "])\n",
    "M = df.values @ TR\n",
    "mx, my = M[:, 0], M[:, 1]\n",
    "\n",
    "mred = red.reshape(-1)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(mx[mred], my[mred], c='crimson', alpha=0.7)\n",
    "ax.scatter(mx[~mred], my[~mred], c='steelblue', alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know know how to project data between dimensions.\n",
    "All we need to do is to create a transform matrix ($T$)\n",
    "with the same number of rows as the number of columns\n",
    "(dimensions, features) in our data and multiply them.\n",
    "In other words, we know how to perform a rather simple\n",
    "dimensionality reduction.\n",
    "\n",
    "The problem is that the numbers inside the matrix $T$\n",
    "shape the projection in different ways.\n",
    "The numbers squeeze, rotate, and move the data around.\n",
    "Moreover, before we had $4096 \\cdot 3 = 12288$ values\n",
    "representing our data now we have $4096 \\cdot 2 = 8192$ values.\n",
    "Therefore we lost some information about our data\n",
    "in the process of the projection.\n",
    "If we want to minimize the amount of information we lose\n",
    "during the projection we must have a way of selecting the\n",
    "best numbers for the values inside $T$.\n",
    "Enters Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA find the best projection that **minimizes the loss of variance**\n",
    "within the data we project.\n",
    "For the time being let us argue that PCA has a way of finding\n",
    "the best possible transformation matrix to minimize the\n",
    "loss in variance across the data.\n",
    "Another detail of the transformation matrix is that the\n",
    "rotation operation that the transformation performs\n",
    "is always around the origin.\n",
    "Hence PCA will move the data so that the mean in every\n",
    "dimension is zero, i.e. that the data is centered at the origin.\n",
    "This way the rotations happen around the middle of the data itself.\n",
    "\n",
    "When used from `sklearn`,\n",
    "PCA behaves just as any other `sklearn` model.\n",
    "PCA expects to be given the number of components we project onto,\n",
    "this is a hyperparameter of the PCA algorithm.\n",
    "We are rather confident that we can take the data above and\n",
    "live with its variance in two dimensions only,\n",
    "therefore we will use $2$ components.\n",
    "\n",
    "Since the $z$ feature is almost fully dependent on the values of $x$ and $y$,\n",
    "the variance in $z$ can be explained by the variance in $x$ and $y$.\n",
    "We expect that PCA will figure that out and flatten out the $z$ feature.\n",
    "PCA is a transformation hence we use the `transform` method\n",
    "(or more exactly `fit_transform` here) to project the data.\n",
    "The result is still the same data but in less dimensions.\n",
    "This can be visualized or passed on to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.c_[gx.reshape(-1), gy.reshape(-1), z.reshape(-1)]\n",
    "pca = PCA(n_components=2)\n",
    "X_new = pca.fit_transform(X)\n",
    "red_new = red.reshape(-1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(X_new[red_new, 0], X_new[red_new, 1], color='crimson', alpha=0.7)\n",
    "ax.scatter(X_new[~red_new, 0], X_new[~red_new, 1], color='steelblue', alpha=0.7)\n",
    "pca.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since we know that the projection is just a matrix multiplication\n",
    "we can perform the PCA by hand ourselves.\n",
    "We just need to move the data so its mean is zero\n",
    "(subtract the mean from the data in every dimensions)\n",
    "and perform the matrix multiplication with the matrix\n",
    "found by PCA during the `fit` procedure.\n",
    "That matrix lives at the `components_` attribute.\n",
    "\n",
    "We need to use the transpose if the matrix PCA finds\n",
    "for historical reasons.\n",
    "Reason we will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_or = df - df.mean()\n",
    "M = df_or.values @ pca.components_.T\n",
    "mx, my = M[:, 0], M[:, 1]\n",
    "\n",
    "mred = red.reshape(-1)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(mx[mred], my[mred], c='crimson', alpha=0.7)\n",
    "ax.scatter(mx[~mred], my[~mred], c='steelblue', alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only secret remaining is how PCA actually find this optimal transform matrix.\n",
    "The algorithm works by decomposing the space into eigenvectors and eigenvalues.\n",
    "Then it reorders the eigenvectors in order of how much variance they explain,\n",
    "i.e. in order of the eigenvalues.\n",
    "\n",
    "It is a common misconception that PCA searches for the eigenvectors and\n",
    "eigenvalues within the data.\n",
    "Eigenvectors and eigenvalues are only defined for square matrices,\n",
    "and PCA perform the following matrix multiplication.\n",
    "\n",
    "$$\n",
    "X_{4096 \\times 3}^T X_{4096 \\times 3}\n",
    "$$\n",
    "\n",
    "Which results in a matrix of $3$ rows and $3$ columns, a square matrix.\n",
    "Remember that $X^T$ is the transpose of the matrix $X$.\n",
    "This resulting matrix is also symmetric across its main diagonal,\n",
    "which gives it the property that all its eigenvectors are\n",
    "perpendicular to each other.\n",
    "A perfect selection of vectors to select from as new dimensions.\n",
    "New dimensions shall be perpendicular to each other in order\n",
    "to not deform the data when projecting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the reason that PCA minimizes the loss in variance is because\n",
    "the matrix result from $X^T X$ is proportional to the covariance\n",
    "matrix of $X$.\n",
    "This is often written.\n",
    "\n",
    "$$\n",
    "X_{4096 \\times 3}^T X_{4096 \\times 3} \\propto cov(X)\n",
    "$$\n",
    "\n",
    "The symbol $\\propto$ means proportional.\n",
    "The covariance matrix of $X$ is the covariance across\n",
    "all combinations of columns of $X$.\n",
    "And the eigenvectors of the covariance matrix\n",
    "point in the direction of maximum variance,\n",
    "whilst within the boundaries of being perpendicular to each other.\n",
    "Each eigenvector then points towards the maximum variance\n",
    "discounting all eigenvectors before it.\n",
    "A square and symmetric covariance matrix of $n$ rows and columns\n",
    "has $n$ real eigenvectors,\n",
    "which ordered in decreasing order of eigenvalues are the projections,\n",
    "are the projections which minimize the loss in variance.\n",
    "\n",
    "Finding the eigenvectors and the eigenvalues from $X^TX$ is a quite\n",
    "complex task, with two iterative algorithms: one to find the\n",
    "eigenvalues and another to find the eigenvectors from the eigenvalues.\n",
    "But the task is made much easier by the fact that computing $X^TX$ is cheap,\n",
    "whilst computing the covariance matrix for $X$ can be very expensive.\n",
    "And since both are proportional to each other their eigenvectors\n",
    "point in the same directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![David Hilbert](ul-hilbert.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ul-hilbert.svg</sup></div>\n",
    "\n",
    "<div style=\"border: 0.3em double teal; border-radius: 0.5em; padding: 0.5em\">\n",
    "    David Hilbert was perhaps the creator of mathematical logic as a discipline.\n",
    "    His work, although conflicting with Gödel's, led to the development of axiomatic mathematics.\n",
    "    It is due to both Gödel and Hilbert that mathematicians today start they statements with\n",
    "    \"If we assume that...\".\n",
    "    Hilbert was also the one to name what we today know and Eigenvalues and Eigenvectors.\n",
    "    The concepts have been known as far back as early work by Maxwell but the importance\n",
    "    to Linear Algebra has been seen by Hilbert.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors exist in the same dimensions as our data.\n",
    "In our case the data is in $3$ dimensions,\n",
    "and $X^TX$ is also a matrix of $3$ columns (and $3$ rows).\n",
    "This allows us to visualize where the eigenvectors lie\n",
    "in the space of our data\n",
    "(assuming that we can visualize that space in the first place that is).\n",
    "\n",
    "The PCA `components_` attribute is not only the transformation matrix\n",
    "but it is also the set of eigenvectors used for the transformation.\n",
    "The matrix and the eigenvectors are one and the same.\n",
    "And since it is common for us to use columns as dimensions,\n",
    "this is the reason we keep the matrix in `components_`\n",
    "with the same number of columns as the original data.\n",
    "And also the reason we needed to use the transpose earlier.\n",
    "The `explained_variance_ratio_` contains the scaled eigenvalues\n",
    "so that they all sum to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(gx[red], gy[red], z[red], c='crimson', alpha=0.2)\n",
    "ax.scatter(gx[~red], gy[~red], z[~red], c='steelblue', alpha=0.2);\n",
    "\n",
    "M = np.c_[gx.mean(), gy.mean(), z.mean()]\n",
    "sizes = pca.explained_variance_ratio_ / pca.explained_variance_ratio_.min()\n",
    "eigen = pca.components_ * sizes[:, np.newaxis]\n",
    "ax.quiver(M[[0, 0], 0], M[[0, 0], 1], M[[0, 0], 2],\n",
    "          eigen[:, 0], eigen[:, 1], eigen[:, 2],\n",
    "          linewidths=3, color='seagreen', length=3, arrow_length_ratio=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the projection can be done in one way,\n",
    "in our case from $3$ to $2$ dimensions,\n",
    "we can perform the inverse projection too.\n",
    "In order to do that we only need to right multiply\n",
    "by the transpose of the transformation matrix.\n",
    "Or, in the case of the PCA implementation by the `components_`\n",
    "attribute itself since it is already transposed.\n",
    "\n",
    "This inverse projection allow us to see what information was lost\n",
    "during the original projection.\n",
    "We cannot recover the information that was lost because it was thrown away.\n",
    "In the top graph we see the original data,\n",
    "whilst in the second we see the data that was projected into $2$\n",
    "dimensions - with information loss - and then projected back to $3$ dimensions.\n",
    "In the second graph the data is flat,\n",
    "most of the variance over the $z$ coordinate has been thrown away.\n",
    "\n",
    "This inverse transformation is available directly on the PCA model,\n",
    "under the `inverse_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_new @ pca.components_\n",
    "mx, my, mz = M[:, 0], M[:, 1], M[:, 2]\n",
    "\n",
    "mred = red.reshape(-1)\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax_u = fig.add_subplot(211, projection='3d')\n",
    "ax_d = fig.add_subplot(212, projection='3d')\n",
    "ax_u.scatter(gx[red], gy[red], z[red], c='crimson', alpha=0.7)\n",
    "ax_u.scatter(gx[~red], gy[~red], z[~red], c='steelblue', alpha=0.7);\n",
    "ax_d.scatter(mx[mred], my[mred], mz[mred], c='crimson', alpha=0.7)\n",
    "ax_d.scatter(mx[~mred], my[~mred], mz[~mred], c='steelblue', alpha=0.7);\n",
    "ax_u.view_init(elev=25., azim=-60)\n",
    "ax_d.view_init(elev=25., azim=-60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity of PCA and manifold techniques\n",
    "\n",
    "PCA is a technique that depends on the linearity of the dimensions it works with.\n",
    "Relations that are non-linear, such as the letter B we saw earlier,\n",
    "cannot be captured by PCA.\n",
    "Covariance and variance are components of correlation,\n",
    "and hence techniques to identify linear relations after all.\n",
    "\n",
    "To deal with non-linear relationships manifold techniques were developed.\n",
    "A manifold is a space that more-or-less look like euclidean space\n",
    "but it is bent in several ways,\n",
    "e.g. a bent sheet of paper in three dimensional space.\n",
    "These techniques are based on maintaining distances between points in the data\n",
    "instead of maintaining the variance within specific dimensions.\n",
    "That said, PCA is easy to interpret due to the cumulative variance,\n",
    "yet it is often difficult to interpret what a manifold technique is doing.\n",
    "Moreover, there is no definitive measure of whether a manifold technique will\n",
    "or will not converge for a particular dataset,\n",
    "and manifold techniques are more sensitive to outliers than PCA.\n",
    "\n",
    "As a rule of thumb,\n",
    "it is wise to attempt manifold techniques only after attempting PCA\n",
    "and understanding the shape of the data.\n",
    "Some manifold techniques include:\n",
    "\n",
    "- Multidimensional Scaling (MDS) is the simplest manifold which works by preserving\n",
    "  (as much as possible) the distances between *all* points in the dataset.\n",
    "\n",
    "- Locally Linear Embedding (LLE) works like MDS but only preserves distances within\n",
    "  a defined number of neighbors, this allows to \"unroll\" certain relationships.\n",
    "\n",
    "- Isomap is similar to LLE in that it uses a neighbor search but then computes\n",
    "  eigenvectors over the local groups.\n",
    "\n",
    "- Spectral Embedding is a stochastic way of building eigenmaps from a graph based\n",
    "  neighbor search, this makes it similar to LLE.\n",
    "\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE) may reveal structures at\n",
    "  different scales since it builds several t-distributions (similar to a bell shape)\n",
    "  within groups of neighbors and then works with these t-distributions instead\n",
    "  of the actual data points."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

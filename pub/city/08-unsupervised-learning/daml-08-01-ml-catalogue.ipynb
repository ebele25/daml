{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08.00 Machine Learning Groupings, Again\n",
    "\n",
    "We saw that we can divide machine learning models in four categories.\n",
    "\n",
    "* Supervised Learning\n",
    "  - Classification\n",
    "  - Regression\n",
    "* Unsupervised Learning\n",
    "  - Dimensionality Reduction\n",
    "  - Clustering\n",
    "\n",
    "And although this grouping is quite outdated we will follow it for the time being.\n",
    "We did skim over classification and regression but more importantly\n",
    "we did see how we evaluate ML models and how we automate the evaluation.\n",
    "Now, one may be surprised that we already went through the majority\n",
    "of way of evaluating an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *unsupervised learning* there are no good ways to know whether a model\n",
    "works well or not.\n",
    "Instead, faced with a problem against which we need to attempt unsupervised learning,\n",
    "we need to attempt to transform at least part of the problem into supervised\n",
    "learning and evaluate it with accuracy scores, f1 measures, and so on.\n",
    "\n",
    "Often we do not have labels for the data we use unsupervised learning on,\n",
    "in such a case any form of model evaluation will be very difficult.\n",
    "Perhaps the easiest way to evaluate a model for a problem for which we have\n",
    "no labels at all and cannot get any labels,\n",
    "is to find a similar problem for which we have labels and train a model\n",
    "on that similar problem.\n",
    "Once we evaluate a good model on the similar problem we assume that\n",
    "it will work in a similar fashion on the original problem.\n",
    "\n",
    "Most of the time with unsupervised learning we have or can get at least\n",
    "a handful of labels.\n",
    "We can then evaluate our unsupervised model on the handful of labels\n",
    "in a similar fashion as we would do for a supervised one.\n",
    "This can be sometimes called *semi-supervised learning*,\n",
    "and is our first indication that the list of ML groupings\n",
    "at the top is quite incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "Will allow us to explore the data in lesser dimensions, whatever we call the dimension.\n",
    "Here we come back to the concept of **distance**.\n",
    "We did see different distances such as L-norms, including `L1` and `L2`,\n",
    "but many other measures can be made to work as a distance:\n",
    "sum of coordinates and polynomial functions on top of the data\n",
    "are distances we have already used;\n",
    "other distances include measures such as cosine similarity or probabilities.\n",
    "Don't worry what these exactly are right now,\n",
    "we will explore some of the ways of measuring distance.\n",
    "\n",
    "Independent of what we use for distance,\n",
    "we say that two points (samples) are **similar** if they are close to one another,\n",
    "and **dissimilar** if they are far apart.\n",
    "\n",
    "One way to evaluate dimensionality reduction is to perform the reduction to a\n",
    "set of numbers we can visualize and then check *by-the-eye* that\n",
    "things close together are the ones we expect to be similar and\n",
    "things far apart are the ones we expect to be dissimilar.\n",
    "\n",
    "When we have a handful of labels we can perform the dimensionality\n",
    "reduction and then train another model on top of the reduced data.\n",
    "The performance of the resulting model will be an indication\n",
    "of the performance of the dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Is similar to the models we have already seen when looking at supervised learning.\n",
    "Instead of changing the structure of the space where the data lives\n",
    "as is done with dimensionality reduction,\n",
    "clustering attempts to identify the structure in the data.\n",
    "The identification of this structure is again performed\n",
    "by means of **distance** in one form or another.\n",
    "\n",
    "One builds *clusters*, structures which then allow new (unseen) data\n",
    "to be classified into one of these clusters.\n",
    "We argue that samples that are **similar** to each other are in a cluster together,\n",
    "and in a different cluster from samples that are **dissimilar** to them.\n",
    "Contrary to supervised learning one such cluster has no meaning,\n",
    "unless we give it one using a guesstimate, *by-the-eye*, evaluation.\n",
    "Without labels clusters are just groupings of data,\n",
    "yet seeing such groupings we may be able to guess labels for them.\n",
    "\n",
    "If we have some labels then we can check what label\n",
    "is present for the majority of samples in a cluster and give the cluster a meaning.\n",
    "In this fashion we can evaluate a clustering model in a similar way\n",
    "to supervised learning models.\n",
    "We can then say that a good clustering would group together the samples according\n",
    "to their labels.\n",
    "\n",
    "When one has no labels at all and cannot guesstimate them there remains\n",
    "possibility of evaluating the shape of the clusters.\n",
    "Clusters live in the space where the data lives.\n",
    "Evaluation techniques can geometrically check whether the clusters are\n",
    "round or dense.\n",
    "And the concept of round or dense can be changed slightly depending\n",
    "of how we measure *distance*.\n",
    "Still, geometric techniques for clustering evaluation are rather limited,\n",
    "and are an area of active research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's import a handful of things that will help us with the next topic.\n",
    "\n",
    "We will need a handful of mathematical concepts,\n",
    "although not necessarily equations,\n",
    "in order to advance from here.\n",
    "We will also have a quick look at a few mathematicians\n",
    "that allowed us to get where we are today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kurt Gödel](ul-godel.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ul-godel.svg</sup></div>\n",
    "\n",
    "<div style=\"border: 0.3em double teal; border-radius: 0.5em; padding: 0.5em\">\n",
    "    Kurt Gödel's incompleteness theorems, from 1931,\n",
    "    were one of the greatest turns in Mathematical thinking.\n",
    "    The theorem's themselves only prove the impossibility of building a full set\n",
    "    of mathematical axioms which would explain the axioms themselves.\n",
    "    Despite the fact that the theorem's only build upon the axioms of arithmetic,\n",
    "    the deepest ideas behind it proved to be revolutionary.\n",
    "    Later works and interpretations of Gödel's theorems built the idea that\n",
    "    mathematics cannot ever be proven by mathematics themselves,\n",
    "    which placed applied mathematics on the same level as \"pure\" theoretical mathematics.\n",
    "    This line of thinking also led to the rejection of the idea that there must\n",
    "    exist a set of first principles for every phenomena.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity\n",
    "\n",
    "For a long time science assumed the linearity of the world.\n",
    "This is another way to say that science believed that every phenomena\n",
    "can be distilled to its first principles and then those first principles\n",
    "will combine in a linear fashion to produce the more complex phenomena.\n",
    "That is a lot of talk that does not actually tell us what this linearity is.\n",
    "It often is the case that the exact definition of linearity is slightly\n",
    "different depending on the discipline one looks at.\n",
    "Instead we will attempt to define where *linearity fails*\n",
    "and work backward from there.\n",
    "\n",
    "On the assumption that one would be capable of dividing the world into\n",
    "all its first principles then, by definition,\n",
    "none of the first principles could be correlated with any of the\n",
    "other first principles.\n",
    "Otherwise, if the first principles are indeed correlated, \n",
    "then one first principle is dependent on the other and hence not\n",
    "a first principle but phenomena caused by the other first principle.\n",
    "\n",
    "Let us play with this concept and build two graphs:\n",
    "the capital letter B, and some white noise - a completely uniform\n",
    "random set of points - and make their histograms in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([\n",
    "    [.1, .9], [.20, .9],\n",
    "              [.23, .87],\n",
    "    [.1, .8], [.25, .8],\n",
    "    [.1, .7], [.25, .7],\n",
    "              [.23, .63],\n",
    "    [.1, .6], [.20, .6],\n",
    "    [.1, .5], [.20, .5],\n",
    "    [.1, .4], [.27, .4],\n",
    "              [.27, .37],\n",
    "    [.1, .3], [.30, .3],\n",
    "              [.27, .23],\n",
    "    [.1, .2], [.27, .2],\n",
    "    [.1, .1], [.20, .1],\n",
    "])\n",
    "noise = []\n",
    "for i in range(70):\n",
    "    noise.append(B + np.random.random(B.shape)*.12)\n",
    "B = np.vstack(noise)\n",
    "white = np.random.random(B.shape)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(16, 9))\n",
    "ax[1, 0].scatter(B[:, 0], B[:, 1], s=20, c='crimson')\n",
    "ax[0, 0].hist(B[:, 0], color='crimson')\n",
    "ax[0, 0].set_title('Letter B, corr %.06f' % np.corrcoef(B[:, 0], B[:, 1])[0, 1])\n",
    "ax[1, 1].hist(B[:, 1], color='crimson', orientation='horizontal')\n",
    "ax[1, 3].scatter(white[:, 0], white[:, 1], s=20, c='steelblue')\n",
    "ax[0, 3].hist(white[:, 0], color='steelblue')\n",
    "ax[0, 3].set_title('White Noise, corr %.06f' % np.corrcoef(white[:, 0], white[:, 1])[0, 1])\n",
    "ax[1, 4].hist(white[:, 1], color='steelblue', orientation='horizontal')\n",
    "for img in ax.flat:\n",
    "    img.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both graphs we only have points in $2$ dimensions,\n",
    "and we evaluated the correlation between the $2$ dimensions in each case.\n",
    "Both the capital letter B and the white noise have pretty much\n",
    "no correlation between the dimensions in which they exist.\n",
    "\n",
    "If we accept the approximation that the correlations are indeed exactly zero,\n",
    "then we can argue that we have separated the world where the letter B\n",
    "and the white noise live into its first principles:\n",
    "the positions on the x-axis and the positions on the y-axis.\n",
    "And since the positions and correlations are pretty much the same\n",
    "the letter B and the white noise are *exactly the same thing*!\n",
    "\n",
    "Obviously they are not the same thing.\n",
    "We can see the letter B because our human brain is very good\n",
    "at finding non-linear patters.\n",
    "When we draw a capital letter B we do not take notice of exactly\n",
    "where we place each part of the letter.\n",
    "Instead we take effort to make sure that the letter B has one\n",
    "straight side and round holes in it.\n",
    "*The exact position of the parts of B does not matter,\n",
    "what matters is the position between the parts of B*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here be Dragons - where Linearity Breaks\n",
    "\n",
    "The letter B has a pattern we can identify with our human brain\n",
    "which do not follow from the definition of the two-dimensional\n",
    "world built by positions of points with two coordinates.\n",
    "Finding such patters as the letter B in the example\n",
    "is a case of a **non-linear problem**.\n",
    "\n",
    "How do we know that a problem is non-linear?\n",
    "Well, we do not.\n",
    "The only way to know that there is more to be seen than\n",
    "what is possible by splitting the entire world where the problem lives\n",
    "into its first principles,\n",
    "is to perform the splitting and attempting to solve the problem.\n",
    "If the problem cannot be solved by splitting it into first\n",
    "(uncorrelated) principles then it is a non-linear problem.\n",
    "With experience one learns to identify non-linear problems.\n",
    "Notably, the world has many more non-linear problems than linear ones.\n",
    "\n",
    "Linearity of problems, even when just an approximation,\n",
    "allows for several nice properties.\n",
    "Linear Algebra and Univariate statistics describe linear problems well,\n",
    "and have several closed form solutions (solutions than can be\n",
    "evaluated straight away).\n",
    "Hence it is worth to first attempt a linear solution to a problem\n",
    "before assuming that it is non-linear.\n",
    "\n",
    "That being said, assuming that everything in the world around us\n",
    "can be explained by splitting it into first principles - and that\n",
    "linear techniques work on every problem - is akin of hitting a\n",
    "square peg into a round hole.\n",
    "In the case above one would just assume that every letter B\n",
    "is just white noise and never find out what B may actually be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Statistics\n",
    "\n",
    "Univariate Statistics, where each random variable\n",
    "(or each feature when thinking about date) is described by a distribution with a singe peak,\n",
    "are not the only form of statistics.\n",
    "There are statistics which account for the possibility of non-linear patterns in the data.\n",
    "Univariate statistics are just more common,\n",
    "since they include the assumption of a Gaussian distribution of the data.\n",
    "And the Central Limit Theorem we mentioned earlier is also a form\n",
    "of univariate statistics since it is a sum of several univariate\n",
    "distributions (distributions with a single peak).\n",
    "Wilcoxon statistics and Tsallis statistics are examples\n",
    "of statistics that account for non-linearities.\n",
    "\n",
    "Whether the commonalty of univariate statistics is due to the fact that\n",
    "linear problems are easier is up to debate.\n",
    "One can argue that the only reason why linear problems are easier is\n",
    "because we have more tools to deal with them.\n",
    "If mathematics has had developed in a different way than it did,\n",
    "perhaps today we would argue that non-linear problems are easy\n",
    "and non-linear problems are hard.\n",
    "That said,\n",
    "we currently have many more tools to deal with linear problems,\n",
    "hence we often argue that for practical purposes\n",
    "linear problems are easy to solve and non-linear ones are hard."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.04 Why Python is the Leading Language in Data Analytics?\n",
    "\n",
    "This is better seen by action rather than words,\n",
    "let's perform a simple analysis and see.\n",
    "Note that we do not have a dataset ready,\n",
    "we will build the dataset on the fly.\n",
    "\n",
    "Every time I watch a *Die Hard* movie I get the impression that there's almost no women acting.\n",
    "In other words, I get the impression that everyone in that film is a\n",
    "**bunch of middle-aged of blokes hitting each other**.\n",
    "We will try to check this statistically: collect data about all five *Die Hard* movies and\n",
    "plot the ratio of actors and actresses in the cast of each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use more imports than normally here,\n",
    "these libraries are present in either the standard library or most scientific distributions of Python.\n",
    "Notably,these are all present in the anaconda distribution.\n",
    "Of course, all of them can be installed with `pip`.\n",
    "A quick outline:\n",
    "\n",
    "- `time`, `functools` and `collections` are Python standard library utilities for, respectively,\n",
    "  time related system calls, functional programming with lists, and extra data structures.\n",
    "\n",
    "- `requests` is a library for processing HTTP calls with a very, very clean API.\n",
    "\n",
    "- `bs4` (Beautiful Soup 4) is a library to construct a DOM tree (through HTML parsers) and\n",
    "  traverse that tree with a simple API.\n",
    "\n",
    "To describe the functionality of each of these libraries or HTML DOM parsing\n",
    "is way beyond our scope.\n",
    "All these libraries are worth investigating but for our purpose\n",
    "we will only use them to get our data from the web into `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import functools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could get the data from a movie database and then work with structured data.\n",
    "Yet, the main objective of this exercise is to work with rather unstructured and dirty data,\n",
    "therefore we will take the HTML data from [Wikipedia][wiki] about these movies.\n",
    "Wikipeida URLs match the title of each movie with underscores instead of spaces.\n",
    "\n",
    "[wiki]: https://en.wikipedia.org/wiki/Main_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Die Hard', 'Die Hard 2',\n",
    "          'Die Hard with a Vengeance', 'Live Free or Die Hard', 'A Good Day to Die Hard']\n",
    "url_base = 'https://en.wikipedia.org'\n",
    "urls = dict([(m, url_base + '/wiki/' + m.replace(' ', '_')) for m in movies])\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![John McClane](da-die-hard.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>da-die-hard.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have some data in there about the cast of each movie but this is not the full cast.\n",
    "That said, we can assume that the actors present on screen most of the time are on the wikipedia page.\n",
    "Let's write out a handful of assumptions that will help us *scope* the answer to our problem:\n",
    "\n",
    "- If an actor or actress appears has a lot screen time he/she is more likely\n",
    "  to appear on the cast list on wikipedia.\n",
    "\n",
    "- This means that by using the cast for the wikipedia pages consistently\n",
    "  we can argue that we have a good model of screen time of the Die Hard movies.\n",
    "  In other words, the most significant the actor is in the movie the most likely\n",
    "  it is that we can get his name (and wikipedia link) from the cast section.\n",
    "\n",
    "- The same is valid for missing data.\n",
    "  The most important is an actor or actress in the movies the most likely\n",
    "  is that his/her wikipedia page will be complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Die Hard - Wikipedia](da-wikipedia-die-hard-cast.png)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>da-wikipedia-die-hard-cast.png</sup></div>\n",
    "\n",
    "[Section from Wikipedia page for the Die Hard movie][diehard]\n",
    "\n",
    "[diehard]: https://en.wikipedia.org/wiki/Die_Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cast sections of the wikipedia pages are HTML lists `<ul>`.\n",
    "We will find those and retrieve all list items which contain a link to a page.\n",
    "We hope that the link will be to the wikipedia page of the actor/actress\n",
    "(most of the time it is).\n",
    "\n",
    "Note: We wait 3 seconds between each call to prevent the wikipedia webservers\n",
    "from kicking us out as an unpolite crawler.\n",
    "On the internet, it is polite to wait a moment between calls to not flood a webserver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cast(url):\n",
    "    r = requests.get(url)\n",
    "    print(r.status_code, url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    cast = soup.find('span', id='Cast').parent.find_all_next('ul')[0].find_all('li')\n",
    "    return dict([(li.find('a')['title'], li.find('a')['href']) for li in cast if li.find('a')])\n",
    "\n",
    "\n",
    "movies_cast = dict([(m, retrieve_cast(urls[m])) for m in movies])\n",
    "movies_cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place the results in a dictionary of dictionaries.\n",
    "For each movie we have the cast and the web page for each person.\n",
    "\n",
    "Now we have the links for each actor/actress but again we need to retrieve data from and HTML page.\n",
    "The *vcard* pane on the right hand side looks promising for data collection:\n",
    "The pane is organized into a key-value structure,\n",
    "and it is an HTML table which is easy to transverse.\n",
    "The data in the pane is still messy but we will worry about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vcard Pane](da-wikipedia-vcard-pane.png)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>da-wikipedia-vcard-pane.png</sup></div>\n",
    "\n",
    "[Section from Wikipedia page on Bruce Willis][willis]\n",
    "\n",
    "[willis]: https://en.wikipedia.org/wiki/Bruce_Willis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's more.\n",
    "Looking at the raw HTML we can notice a handful data elements that are not displayed\n",
    "but which may be useful.  One such element is `bday`,\n",
    "present in the wikipedia pages of several people,\n",
    "including some of the actors and actresses we are after.\n",
    "Since we are looking for *middle-aged blokes* we should retrieve this data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hidden bday](da-wikipedia-hidden-bday.png)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>da-wikipedia-hidden-bday.png</sup></div>\n",
    "\n",
    "Hidden element on Wikipedia page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that said, we cannot forget that Wikipedia is not a golden source for data.\n",
    "Data will be dirty, and we will find pages where the *vcard* section is not present,\n",
    "and it is likely we will not be able to retrieve any data from such pages.\n",
    "One of our assumptions is that the actors and actresses with most screen time\n",
    "will have the most complete wikipedia pages,\n",
    "therefore we should be able to retrieve the most important data.\n",
    "\n",
    "We also need to think how we are going to structure this data.\n",
    "We have the cast of each movie but several actors appear in more than one movie.\n",
    "Let's add to the data about the actor the movies he has played in.\n",
    "Each time we evaluate the cast of one of the movies we stamp each\n",
    "actor/actress with the movie title.\n",
    "\n",
    "Since we are using a polite timer, this will take a while to run.\n",
    "Here the `retrieve_actor` procedure will give us all mappings\n",
    "(a dictionary) we can find in the right hand pane and the `bday`.\n",
    "We walk the `movies_cast` dictionary of dictionaries and\n",
    "retrieve the data for each person,\n",
    "populating a new dictionary of dictionaries `cast`.\n",
    "In the process we mark the movies in which the cast member acted in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_actor(url):\n",
    "    full_url = url_base + url\n",
    "    r = requests.get(full_url)\n",
    "    print(r.status_code, full_url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    data = {}\n",
    "    bday = soup.find(class_='bday')\n",
    "    if bday:\n",
    "        data['bday'] = bday.string\n",
    "    vcard = soup.find('table', class_='vcard')\n",
    "    if vcard:\n",
    "        ths = vcard.find_all('th', scope='row')\n",
    "        th_rows = [th.text.replace('\\xa0', ' ') for th in ths]\n",
    "        th_data = [th.find_next('td').text.replace('\\xa0', ' ') for th in ths]\n",
    "        data.update(dict(zip(th_rows, th_data)))\n",
    "    return data\n",
    "\n",
    "\n",
    "cast = {}\n",
    "for m in movies_cast:\n",
    "    for act in movies_cast[m]:\n",
    "        data = retrieve_actor(movies_cast[m][act])\n",
    "        data[m] = 1\n",
    "        if data and act in cast:\n",
    "            cast[act].update(data)\n",
    "        elif data:\n",
    "            cast[act] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the data!\n",
    "\n",
    "Well, yes, we got *some* data, but that does not yet make it useful data.\n",
    "First of all we need to try to figure out what kind of values we have.\n",
    "The *vcard* pane that we parsed was a key-value table,\n",
    "we should look at what keys we have.\n",
    "The trick with `set` and `reduce` is just a quick functional\n",
    "way to write a loop looking for unique keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [list(cast[i]) for i in cast]\n",
    "flat_keys = set(functools.reduce(lambda x, y: x + y, keys))\n",
    "sorted(flat_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appears to be good enough.\n",
    "Some keys are quite clear as to what they represent (e.g. \"Occupation\"),\n",
    "others are quite elusive (e.g. \"Genres\").\n",
    "One way or another we have a list of keys which we can turn into columns\n",
    "of a data frame.\n",
    "We need to figure out what is the set of all keys in order\n",
    "to know how many columns the data frame will need to have.\n",
    "\n",
    "We build two data structures:\n",
    "One a list of the actor and actress names, which we will use as the data frame index.\n",
    "And the other data structure as a list of values for every key for every actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = []\n",
    "df_columns = {}\n",
    "for k in flat_keys:\n",
    "    df_columns[k] = []\n",
    "for act in cast:\n",
    "    df_name.append(act)\n",
    "    for k in df_columns:\n",
    "        if k in cast[act]:\n",
    "            df_columns[k].append(cast[act][k])\n",
    "        else:\n",
    "            df_columns[k].append(np.nan)\n",
    "\n",
    "df = pd.DataFrame(df_columns, index=df_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can operate on this data in `pandas` directly.\n",
    "We see a lot of `NaN`s but we can deal with missing easily.\n",
    "\n",
    "For the time being let us save our work so we do not lose it.\n",
    "The `to_csv` procedure will save the data including the position\n",
    "of the missing values.\n",
    "We also give the index a name so when loading the data we can\n",
    "specify the index to parse by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = 'webpage_name'\n",
    "df.to_csv('da-die-hard-newest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data collected from a changeable source such as Wikipedia\n",
    "is different every time we collect it.\n",
    "The filename used here is different from the one in the next section\n",
    "where we load the data.\n",
    "That is in order to preserve reproducibility.\n",
    "\n",
    "If you want to perform the same analysis on the more actual data\n",
    "you have collected, feel free to change the filename.\n",
    "Note that several things will look different from here on in that case.\n",
    "And you are the only one who can deal with them since\n",
    "you are the only one with the data you just collected."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

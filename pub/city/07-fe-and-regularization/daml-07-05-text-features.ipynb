{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.04 Text Features\n",
    "\n",
    "Machine Learning deals only with numeric data\n",
    "but not all data in the world is numeric.\n",
    "Two examples of non-numerical data that is meaningful for learning are:\n",
    "categorical features (e.g. animal species) and plain text (e.g. product reviews).\n",
    "There are tricks that allow us to deal with non-numerical data,\n",
    "these tricks are part of *feature engineering*.\n",
    "\n",
    "For a start let's import a handful of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection\n",
    "\n",
    "Dealing with non-numerical data is only a part of feature engineering,\n",
    "although is often the most common application that is called by this name.\n",
    "Actually feature engineering is not a collection of techniques\n",
    "but a generic name to define tasks performed around input data to our model.\n",
    "These include:\n",
    "\n",
    "- Modifying existing features - e.g. scaling\n",
    "- Selecting only a subset of features - e.g. removing highly correlated features\n",
    "- Building new features from existing ones - e.g. squaring features to get only positive values\n",
    "- Encoding features in a different representation - e.g. one-hot-encoding\n",
    "- Learning new features from data - e.g. huge neural networks fed with lots of data\n",
    "\n",
    "The last example requires really huge amounts of data,\n",
    "hundreds of millions of samples.\n",
    "\n",
    "There are plethora of texts on feature engineering an selection but perhaps\n",
    "the most important idea about building and selecting features in the current\n",
    "day and age is that **no one has authoritative answers on feature engineering**.\n",
    "There are well proven statistical techniques for feature selection\n",
    "but these require the knowledge of complete correlations across all\n",
    "combinations of features, often an infeasible task.\n",
    "Moreover these selection techniques are proven for linear relations\n",
    "across data and outputs,\n",
    "it has been proven that such techniques *lose information*\n",
    "about non-linear relations within the data.\n",
    "\n",
    "The bottom line of feature engineering is that in the present times\n",
    "there is no single or combination of techniques that can substitute\n",
    "a human being and his capacity of understanding the data.\n",
    "Instead of relying on feature engineering techniques to build or\n",
    "select features one uses these techniques and builds sample models\n",
    "in order to gain understanding about the data.\n",
    "Then one uses that information about the data to construct his own\n",
    "pipelines feeding the data into the model.\n",
    "Despite advances, at the time of writing, feature engineering\n",
    "is more of a black art rather than engineering - and it is likely\n",
    "to stay that way for some time still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of building an authoritative method for extraction of good features\n",
    "from data for ML use.\n",
    "We will describe some common techniques and describe\n",
    "their meaning over the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "When we deal with the proper names of things or people we are most often dealing\n",
    "with categorical data.\n",
    "One example of such would be the list of British Cities we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = ['Liverpool', 'Manchester', 'Cardiff']\n",
    "country = ['England', 'England', 'Wales']\n",
    "population2001 = [435500, 405300, 305353]\n",
    "df = pd.DataFrame({'city': city,\n",
    "                   'country': country,\n",
    "                   'population 2001': population2001,\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can force the city and country columns to have a categorical type.\n",
    "In `pandas` that is a way of assigning numbers to a column,\n",
    "these numbers then reference a set of categorical labels.\n",
    "\n",
    "This also means that the data now is completely numerical.\n",
    "We can take the numbers of categories and assign them to\n",
    "the columns.\n",
    "In `pandas` these are stored in `cat.codes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = df.copy()\n",
    "cat_df.city = cat_df.city.astype('category')\n",
    "cat_df.city = cat_df.city.cat.codes\n",
    "cat_df.country = cat_df.country.astype('category')\n",
    "cat_df.country = cat_df.country.cat.codes\n",
    "cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, that is *not* enough.\n",
    "Numerical values have an order, therefore we can test for inequality.\n",
    "Based on the data above we can say that:\n",
    "\n",
    "$$\\texttt{England} < \\texttt{Wales}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\texttt{Manchester} > \\texttt{Liverpool}$$\n",
    "\n",
    "Unfortunately, apart from their use in rugby of football jokes,\n",
    "these inequalities are rather useless.\n",
    "Moreover, these inequalities are likely to confuse an ML algorithm.\n",
    "Instead we need to encode the data into a form that would disallow\n",
    "such inequalities one such form is called *one-hot-encoding*.\n",
    "In one-hot-encoding each sample has several features built from the categorical feature\n",
    "but only one of the columns contain a one, all other columns contain zeros.\n",
    "\n",
    "In `pandas` `get_dummies` exists for this exact purpose,\n",
    "to build a one-hot-encoding from a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, prefix_sep='=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is data that we can feed into an ML technique without worrying about confusing it.\n",
    "That said, this representation can use huge amounts of memory if there is a big number of features.\n",
    "To alleviate the memory problem `sklearn` can perform one-hot-encoding on sparse matrices (from `scipy`),\n",
    "this way we only need to store the ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Data\n",
    "\n",
    "Plain, unorganized, text data present different challenges to transform into a numeric representation.\n",
    "For a start we cannot just one-hot-encode words because they may appear more than once in each sample.\n",
    "We could encode the presence of words in each sample but when distinguishing between samples\n",
    "certain words are certainly more important than others, e.g. we can safely assume that\n",
    "the word \"the\" will appear in almost every sample.\n",
    "\n",
    "Let's try to encode some sentences in a reasonable fashion.\n",
    "And let us try to keep as much of the sentence meaning as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'quick brown fox',\n",
    "    'brown dog',\n",
    "    'gray fox',\n",
    "    'gray dog',\n",
    "    'fox',\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quick Brown Fox](fe-fox.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>fe-fox.svg</sup></div>\n",
    "\n",
    "<div style=\"border: 0.5em double teal; border-radius: 0.5em; padding: 0.5em;\">\n",
    "    The phrase \"The quick brown fox jumps over the lazy dog\" is used to show characters\n",
    "    of fonts since the printing press times.\n",
    "    The sentence contains all letters used in the English alphabet and hence\n",
    "    is a good visual evaluation of the shape of every letter in a font.\n",
    "    This is contrary to \"Lorem ipsum\" which has little meaning.\n",
    "    Whether \"Lorem ipsum\" text has been intentionally or accidentally\n",
    "    altered to read as nonsensical Latin is still up for discussion.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a start we may attempt to just count distinct words in ever sentence.\n",
    "We need two passes, one to figure out all available words\n",
    "and one to count these words in every sentence.\n",
    "\n",
    "For simplicity we use Python dictionaries to build columns\n",
    "and then transpose the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "all_words = reduce(lambda x, y: x + ' ' + y, sentences).split()\n",
    "word_count = dict((k, all_words.count(k)) for k in sorted(all_words))\n",
    "print('FULL WORD COUNT:', word_count)\n",
    "print()\n",
    "\n",
    "d = {}\n",
    "for s in sentences:\n",
    "    d[s] = {w: s.count(w) for w in word_count}\n",
    "pd.DataFrame(d).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a viable encoding, and it has the name *term frequency* or just *TF*.\n",
    "But we can do better.\n",
    "\n",
    "Let's notice that in the column \"quick\" appears only in the first sentence,\n",
    "the word \"fox\" appears in the majority of sentences,\n",
    "and the remaining words appear in a similar fashion across the corpus.\n",
    "Where *corpus* is a just a fancy name for a dataset composed of text documents.\n",
    "From our analysis, if we use the word \"quick\" we can identify the sentence\n",
    "straight away but if we use the word \"fox\" we need more information.\n",
    "This is the idea of more important words we alluded to earlier.\n",
    "\n",
    "The word \"quick\" caries more decision value than the word \"fox\".\n",
    "And we can see that these *decision values* are the inverse of the total word counts,\n",
    "where \"fox\" is the most common word and \"quick\" the rarest.\n",
    "The idea that comes to mind is that if we adjust the values\n",
    "in our data frame by the *inverses of the total word counts*\n",
    "we will carry the decision values into our encoding.\n",
    "This idea is called the *inverse document frequency* or *IDF* for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for s in sentences:\n",
    "    d[s] = {w: s.count(w) * (1/word_count[w]) for w in word_count}\n",
    "df = pd.DataFrame(d).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks even better but there are still some problems.\n",
    "We did place our sentences as points in $5$ dimensional space,\n",
    "where the dimensions are the values of the amount of\n",
    "brown, dog, fox, gray or quick in the sentence.\n",
    "This is often called a word vector space.\n",
    "The issue is that each point is at a different distance from the origin,\n",
    "and if we measure distances between the points then long\n",
    "sentences will be far from short sentences simply due to the number\n",
    "of words in the sentence.\n",
    "Making long sentences and short sentences far apart may be desirable\n",
    "in some circumstances but we could simply count all words in that case\n",
    "and not even bother with machine learning or sentence meaning.\n",
    "\n",
    "To reduce the sentence length problem we can simply normalize\n",
    "the point coordinates.\n",
    "This is often said to be vector normalization,\n",
    "understand point coordinates as a vector and divide by vector length.\n",
    "The resulting vector has length $1$.\n",
    "\n",
    "$$\n",
    "v_n = \\frac{v}{\\Vert v \\Vert}\n",
    "$$\n",
    "\n",
    "Which simply means divide by the square root of the sum of squares,\n",
    "i.e. by the euclidean distance we have seen so many times now.\n",
    "The `div` method on a data frame allows us to specify the axis of division,\n",
    "it just calls `np.divide` behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.div(np.sqrt((df**2).sum(axis=1)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word Representation](fe-quick-brown-fox.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>fe-quick-brown-fox.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this technique is called *TF-IDF* (term frequency inverse document frequency)\n",
    "and can hold a good deal of the meaning of a sentence whilst it allows\n",
    "for numerical processing in ML algorithms.\n",
    "\n",
    "In `sklearn` *TF-IDF* can be performed with the `TfidfVectorizer`.\n",
    "It uses sparse matrices by default,\n",
    "which we can convert back to NumPy arrays for display with `dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tr = TfidfVectorizer()\n",
    "sparse = tr.fit_transform(sentences)\n",
    "sparse.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you paid attention then you noticed that only some of the numbers\n",
    "from `TfidfVectorizer` match our own attempt at TF-IDF.\n",
    "The TF-IDF technique was originally proposed in search engine research,\n",
    "where we have several way to achieve TF and IDF values.\n",
    "By default `sklearn` uses a logarithmic IDF with smoothing.\n",
    "\n",
    "In `sklearn` for each document we have for each term.\n",
    "\n",
    "$$\n",
    "v = tf \\cdot idf\n",
    "$$\n",
    "\n",
    "Just like in our implementation.  But $idf$ is defined as.\n",
    "\n",
    "$$\n",
    "idf = 1 + log \\frac{1 + n}{1 + df}\n",
    "$$\n",
    "\n",
    "Where the $1$ at the beginning is called the smoothing\n",
    "$df$ is the count of documents with the word - just like out full word count,\n",
    "and $n$ the number of documents - sentences in our case.\n",
    "We can adapt our own implementation to perform TF-IDF in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "n = len(sentences)\n",
    "for s in sentences:\n",
    "    d[s] = {w: s.count(w) * (1 + np.log( (1 + n) / (1 + word_count[w]))) for w in word_count}\n",
    "df = pd.DataFrame(d).T\n",
    "df.div(np.sqrt((df**2).sum(axis=1)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we match the numbers from `sklearn`'s `TfidfVectorizer`.\n",
    "\n",
    "In summary we can argue the TF-IDF is a technique to transform\n",
    "sentences or even bigger documents into numbers and still\n",
    "keep some of the meaning of those documents.\n",
    "In the technique $tf$ is how often a term appears in the document,\n",
    "and $df$ is how many documents contain the term.\n",
    "\n",
    "Note that our own implementation is rather primitive.\n",
    "We did not account for the problem\n",
    "of duplicate words within the same document.\n",
    "And we did not account for punctuation in documents.\n",
    "This is because our sentences were rather simple.\n",
    "The `sklearn` implementation accounts for duplicate words\n",
    "by making three passes instead of two.\n",
    "Then it performs tokenization more complex than our use of `split`;\n",
    "tokenization is the way of separating strings - be it sentences or full\n",
    "documents - into individual tokens (words).\n",
    "\n",
    "Finally there also exists the problem of *stemming*:\n",
    "where \"is not\" and \"isn't\" have the same meaning despite the fact\n",
    "that are written differently.\n",
    "Moreover ends of words may change without changing the concrete meaning\n",
    "of the text, for example, whether one uses \"do\", \"done\" or \"doing\"\n",
    "has little difference for our vector space but is again written in different ways.\n",
    "Stemming performs several linguistic tricks to reduce words to their stems,\n",
    "pure meanings, without the conjugation.\n",
    "In `sklearn` we do *not* account for stemming but other libraries such as\n",
    "`nltk` have well tested stemmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Term frequency and weighting - Introduction to Information Retrieval][1]\n",
    "- [Weighting Schemes - Introduction to Information Retrieval][2]\n",
    "\n",
    "[1]: https://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html \"TF-IDF\"\n",
    "[2]: https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html \"Weighting\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

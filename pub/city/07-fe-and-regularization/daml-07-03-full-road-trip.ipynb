{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.02 Full Road Trip\n",
    "\n",
    "Let's move forward: can we estimate the speed at each point on a longer strip of a road?\n",
    "For example, some 30km of a road with turns and inclines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Formula One](fe-formula-one.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>fe-formula-one.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the common suspects and make a short simulation of a road trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simulation it is interesting to note that most problems in the real world are not linear,\n",
    "they are either exponential or periodic.  Why?  Something, something, complexity theory.\n",
    "Anyway, speed on a road is a periodic problem, one speeds up and down in response\n",
    "to the shape of the road he drives on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 30*np.random.rand(256)\n",
    "spd = 13*np.sin(t/2) + 3.7*np.cos(t/2+7) + 3*t + 0.1*(t-10)**2 - 3*(t-3) + 7 + 2.3*np.random.randn(*t.shape)\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.plot(t, spd, 'o', color='crimson')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty difficult to figure out what polynomial degree we need for this fit.\n",
    "But let's try a guess, degree 5.\n",
    "\n",
    "For the time being we will not worry about a test set or cross validation,\n",
    "we will just explore the data.\n",
    "Once we know something about the problem we will come back and\n",
    "perform proper model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that went pretty badly.  This is a case where out model **underfits** the data,\n",
    "i.e. our model has not enough complexity to model the complexity we see.\n",
    "We can also say that our model has too much **bias** about how the data looks.\n",
    "\n",
    "Let us try with a big degree, e.g. 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=100), LinearRegression())\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That ain't good either.\n",
    "On the left hand side we passed the point where we can bend\n",
    "the polynomial and our parameters mess with each other.\n",
    "On the right hand side the polynomial **overfits** the data;\n",
    "or we say that the model has too much **variance**.\n",
    "There are two different examples of *overfitting* on the graph.\n",
    "\n",
    "On the *right* we see a classical case where the function has\n",
    "too much variance.\n",
    "In such a case the parameters will attempt to move the function\n",
    "directly through every single point,\n",
    "and fail to generate a reasonable function across the points\n",
    "in pretty much all cases.\n",
    "\n",
    "On the *left* the linear regression did attempt to find good parameters\n",
    "for several degrees but every time it increased a parameter to, say,\n",
    "$t^{36}$ in order to make the function go up in a specific place it\n",
    "did affect the other places around the function.\n",
    "In turn the function attempted to change the parameter for, say,\n",
    "$t^{47}$ in order to counter the problems generated by the previous\n",
    "parameter increase but this again backfired by changing the function\n",
    "value in other places.\n",
    "The issue happens because linear regression assumes that the is no\n",
    "relationship between the different variables to which the parameters\n",
    "are multiplied.\n",
    "But here $t^{36}$ is very related to $t^{47}$, and also to $t^{63}$,\n",
    "and to pretty much every other value from polynomial features.\n",
    "\n",
    "There is more than one way to solve this problem.\n",
    "Let's see one way: model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias versus Variance - Model Selection\n",
    "\n",
    "To solve the right hand side problem we need to tune of *hyperparameter*,\n",
    "the degree of our polynomial.\n",
    "And then we hope that the left hand side get solved due to a smaller\n",
    "number of parameters.\n",
    "In the case where the left hand side does not get solved then we need to\n",
    "look into regularization - which we will see next.\n",
    "\n",
    "Until now we have been guessing and doing this by hand\n",
    "but trying all values between $5$ and $100$\n",
    "by hand does not seem like a good way of spending an afternoon.\n",
    "Instead `sklearn` can automate this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model for every degree between $5$ and $100$ and evaluate,\n",
    "by cross-validation, which model performs better.\n",
    "`sklearn` provides us with a grid search algorithm,\n",
    "which will perform the training and cross-validating of our model\n",
    "for all hyperparameter values given to it.\n",
    "The `GridSearchCV` is another `sklearn` object\n",
    "that takes `sklearn` objects and gives out a similar interface.\n",
    "When one performs `fit` on a grid search object,\n",
    "it will train models with all possible combinations given\n",
    "in the grid of *hyperparameters*.\n",
    "It then performs cross validation on every model\n",
    "and chooses the model which has the best mean cross validation score.\n",
    "The cross validation argument we use `cv=5` is the `sklearn`'s default.\n",
    "It performs $5$ fold splits *without shuffling* the data,\n",
    "in order to perform a shuffle one can use a `KFold`\n",
    "in the same fashion as with `cross_val_score`.\n",
    "\n",
    "Here we define the grid search to train $95$ linear regression models,\n",
    "each with a different value for the degree of the polynomial features\n",
    "given to the regression itself.\n",
    "We give into the grid search a model that itself is a pipeline of models.\n",
    "That is no issue for the grid search,\n",
    "note how can we specify the element of the pipeline to which\n",
    "the hyperparameter range is to be applied by using a double underscore.\n",
    "We use `polynomialfeatures__degree` to refer to the `degree=`\n",
    "hyperparameter of the `PolynomialFeatures` object,\n",
    "inside the pipeline.\n",
    "And the pipeline itself allows for more shortcuts\n",
    "to the contained objects with the `named_steps` attribute.\n",
    "\n",
    "We are starting to automate hyperparameter selection.\n",
    "Once a best set of hyperparameters is identified by the grid search,\n",
    "it trains a model with these hyperparameter values and all the data.\n",
    "This trained model is then available under `best_estimator_`,\n",
    "and the cross validation mean score that allowed the selection\n",
    "of this model as best under `best_score_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())\n",
    "grid = GridSearchCV(model,\n",
    "                    {'polynomialfeatures__degree': list(range(5, 101))},\n",
    "                    cv=5)\n",
    "grid.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "best = grid.best_estimator_\n",
    "\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = best.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "best, best.named_steps.linearregression.intercept_, best.named_steps.linearregression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we should have a look at the `R2` of the best estimator we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often simple *hyperparameter* tuning is enough to solve even complex problems.\n",
    "Yet, the problem we saw on the left hand side was a problem of dependence between dimensions,\n",
    "and sometimes it cannot be easily solved.\n",
    "We shall look at more techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra References\n",
    "\n",
    "- [Underfitting and Overfitting - SciKit Learn Documentation][1]\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html \"Underfitting and Overfitting\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

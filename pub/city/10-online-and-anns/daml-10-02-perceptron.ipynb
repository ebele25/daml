{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.02 Perceptron\n",
    "\n",
    "The perceptron is a quite old idea.\n",
    "It was born as one of the alternatives for electronic gates\n",
    "but computers with perceptron gates have never been built.\n",
    "Instead, a perceptron is a very good model for online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron receives an arbitrary number of inputs,\n",
    "multiplies each by a *weight*,\n",
    "sums them, and then applies an *activation function* to the result of this sum.\n",
    "Given the number of inputs as the vector $x$, the\n",
    "weights as the vector $w$, and the activation function as $a$;\n",
    "For a single prediction all the perceptron does is:\n",
    "\n",
    "$$\\hat{y} = a \\left( \\sum w^T \\cdot x \\right)$$\n",
    "\n",
    "Inputs to a perceptron should be normalized either between -1 and 1 or 0 and 1,\n",
    "which may result in very small input sizes.\n",
    "Note that the vector $x$ has actually one extra input that is always of unit size.\n",
    "This is called a bias term, and is similar to an amplifier in electronics.\n",
    "This bias term ensures that the outputs out of the perceptron are not bound by,\n",
    "possibly small, input values.\n",
    "\n",
    "To train a perceptron we need to find the correct vector $w$\n",
    "of weights that gives the best classification.\n",
    "There are two ways of doing it:\n",
    "\n",
    "- We can use Gradient Descent (or other simple optimization) to find this vector.\n",
    "- We can perturb each weight by the size of it input and the error of the classification\n",
    "\n",
    "The second option is much cheaper computationally,\n",
    "therefore the most common perceptron learning rule is:\n",
    "\n",
    "$$w_{i+1} = w_i + \\eta(y - \\hat{y})x_i$$\n",
    "\n",
    "Where $\\eta$ is te learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron](ann-perceptron-activation.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ann-perceptron-activation.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $x_1$, $x_2$ and $x_3$ are the features of the data,\n",
    "and $w_n$ the parts of the perceptron vector.\n",
    "And $x'$ is the output of the perceptron, this is the decision function of the model.\n",
    "The most common activation function for a perceptron is the $sign$ function,\n",
    "therefore the perceptron becomes a **binary classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electronic Gates\n",
    "\n",
    "We will attempt to learn boolean logic with our perceptron,\n",
    "this was the original idea for the use of perceptrons\n",
    "but they were implemented in hardware, not software.\n",
    "The interesting part about this classification is that we can generate\n",
    "the full set of data (not a sample) that will pass through our model,\n",
    "therefore we can achieve 100% F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0, 0],\n",
    "                 [0, 1],\n",
    "                 [1, 0],\n",
    "                 [1, 1]])\n",
    "y_or = data.any(axis=1).astype(np.int)\n",
    "y_and = data.all(axis=1).astype(np.int)\n",
    "y_nand = (~data.all(axis=1)).astype(np.int)\n",
    "y_xor = (data[:, 0] != data[:, 1]).astype(np.int)\n",
    "df_true = pd.DataFrame({'left': data[:, 0], 'rigth': data[:, 1],\n",
    "                        'OR': y_or, 'AND': y_and, 'XOR': y_xor, 'NAND': y_nand},\n",
    "                       columns=['left', 'rigth', 'OR', 'XOR', 'AND', 'NAND'])\n",
    "df_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have a full truth table with the correct outputs to each logic function.\n",
    "\n",
    "Just like any other model in `sklearn` we can follow\n",
    "the initialize and `fit` method to train our perceptron.\n",
    "We then feed the inputs and expected outputs for each logic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "gates = {'or': y_or, 'and': y_and, 'xor': y_xor, 'nand': y_nand}\n",
    "test = np.array([\n",
    "    [0, 0], [0, 1], [1, 0], [1, 1],\n",
    "    [1, 0], [0, 1], [1, 1], [0, 0],\n",
    "    [1, 1], [1, 0], [0, 1], [0, 0],\n",
    "])\n",
    "test_y = {\n",
    "    'or': test.any(axis=1).astype(np.int),\n",
    "    'and': test.all(axis=1).astype(np.int),\n",
    "    'xor': (test[:, 0] != test[:, 1]).astype(np.int),\n",
    "    'nand': (~test.all(axis=1)).astype(np.int)\n",
    "}\n",
    "all_preds = test.copy()\n",
    "for g in ['or', 'and', 'nand', 'xor']:\n",
    "    pcpt = Perceptron(penalty='l2', max_iter=10)\n",
    "    pcpt.fit(data, gates[g])\n",
    "    y_pred = pcpt.predict(test)\n",
    "    all_preds = np.c_[all_preds, y_pred[:, np.newaxis]]\n",
    "    f1 = f1_score(test_y[g], y_pred)\n",
    "    print('###### Gate:', g.upper(), 'F1:', f1)\n",
    "pd.DataFrame(all_preds, columns=['left', 'right', 'OR', 'AND', 'NAND', 'XOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron Decision](ann-perceptron-decision.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ann-perceptron-decision.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "We cannot learn XOR with a single perceptron, why is that?\n",
    "The perceptron is a linear model and XOR is not a linear function.\n",
    "The activation function of the perceptron that we are using is the `sign` function,\n",
    "which is a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-2, 0, 10)\n",
    "x2 = np.linspace(0, 2, 10)\n",
    "y1 = x1*0 - 1\n",
    "y2 = x2*0 + 1\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(np.hstack([x1, x2]), np.hstack([y1, y2]), color='crimson');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet we have more tricks in our sleeves, we can use more than one perceptron.\n",
    "We know that:\n",
    "\n",
    "$$\\text{XOR}(a, b) = \\text{AND}(\\text{NAND}(a, b), \\text{OR}(a, b))$$\n",
    "\n",
    "Therefore we can build a XOR model with three perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcpt_or = Perceptron(penalty='l2', max_iter=1e6)\n",
    "pcpt_or.fit(data, y_or)\n",
    "pcpt_nand = Perceptron(penalty='l2', max_iter=1e6)\n",
    "pcpt_nand.fit(data, y_nand)\n",
    "pcpt_and = Perceptron(penalty='l2', max_iter=1e6)\n",
    "pcpt_and.fit(data, y_and)\n",
    "\n",
    "\n",
    "def predict_xor(x):\n",
    "    left = pcpt_or.predict(x)\n",
    "    right = pcpt_nand.predict(x)\n",
    "    return pcpt_and.predict(np.array(list(zip(left, right))))\n",
    "\n",
    "\n",
    "y_pred = predict_xor(test)\n",
    "f1_score(test_y['xor'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron XOR](ann-perceptron-xor.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ann-perceptron-xor.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three perceptrons together can be thought off as a simple Neural Network.\n",
    "The difference is that here we knew exactly how to train every perceptron separately,\n",
    "which is not as easy in (possibly) complex neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

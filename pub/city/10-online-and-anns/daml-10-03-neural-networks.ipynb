{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.03 Neural Networks\n",
    "\n",
    "If we place several perceptrons (neurons) together\n",
    "we can slice the search space with several lines.\n",
    "This is called an *Artificial Neural Network* (ANN)\n",
    "It is not trivial to train these neurons,\n",
    "after all we do not know what the output of most neurons should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just use `sklearn`'s `MLPClassifier` (multi-layer-perceptron-classifier)\n",
    "as a model and see if we can make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lieutenant Columbo](ann-columbo.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ann-columbo.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glass Dataset\n",
    "\n",
    "The forensics of glass composition can reveal the provenience of a piece of glass,\n",
    "yet different brands of glass are slightly different from each other.\n",
    "We will build a model which will classify glass based on its composition,\n",
    "and will use online learning so that we are ready to learn from new data at any time.\n",
    "\n",
    "This is a dataset at UCI machine learning repository, we need to build our `load_glass` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "def load_glass():\n",
    "    glass_dir = 'uci_glass'\n",
    "    data_dir = datasets.get_data_home()\n",
    "    data_path = os.path.join(data_dir, glass_dir, 'glass.data')\n",
    "    descr_path = os.path.join(data_dir, glass_dir, 'glass.names')\n",
    "    glass_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "    glass_descr = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.names'\n",
    "    os.makedirs(os.path.join(data_dir, glass_dir), exist_ok=True)\n",
    "    try:\n",
    "        with open(descr_path, 'r') as f:\n",
    "            descr = f.read()\n",
    "    except IOError:\n",
    "        print('Downloading file from', glass_descr, file=sys.stderr)\n",
    "        r = requests.get(glass_descr)\n",
    "        with open(descr_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        descr = r.text\n",
    "        r.close()\n",
    "    try:\n",
    "        data = np.loadtxt(data_path, delimiter=',')\n",
    "    except IOError:\n",
    "        print('Downloading file from', glass_data, file=sys.stderr)\n",
    "        r = requests.get(glass_data)\n",
    "        with open(data_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        r.close()\n",
    "        data = np.loadtxt(data_path, delimiter=',')\n",
    "    target = data[:, 10].astype(np.int).copy()\n",
    "    target[target > 3] -= 1  # fix non-existent classes\n",
    "    target -= 1              # fix class numbering\n",
    "    return Bunch(DESCR=descr,\n",
    "                 data=data[:, :10].copy(),\n",
    "                 feature_names=['ID', 'RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'],\n",
    "                 target=target,\n",
    "                 target_names=['windows_float_processed',\n",
    "                               'windows_non_float_processed',\n",
    "                               'vehicle_windows',\n",
    "                               'containers',\n",
    "                               'tableware',\n",
    "                               'headlamps'])\n",
    "\n",
    "\n",
    "glass = load_glass()\n",
    "print(glass.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support of the classes is quite different.\n",
    "This isn't an easy problem to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(glass.target).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without knowing much about this model,\n",
    "let's try to use a neural network to classify this dataset.\n",
    "We know that the neural network is a good amount\n",
    "of interconnected perceptrons and that we perturb the perceptron weights\n",
    "based on errors in classification to achieve convergence.\n",
    "\n",
    "We also know that `sklearn` provides us with a simple neural network class,\n",
    "no harm in trying it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(glass.data, glass.target, test_size=0.2, stratify=glass.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a real dataset,\n",
    "we may as well try to do things properly and take a test set out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "net = MLPClassifier(activation='relu', hidden_layer_sizes=(20,),\n",
    "                    alpha=0.01, tol=0.001,\n",
    "                    max_iter=200, solver='sgd')\n",
    "param_dict = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'tol': [0.001, 0.01, 0.1],\n",
    "}\n",
    "grid = GridSearchCV(net, param_dict, cv=9)\n",
    "grid.fit(xtrain, ytrain)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models may not converge but the cross-validation should root them out.\n",
    "And the best model should be a converged network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score isn't bad but a classification report may give a better view\n",
    "of how a multilabel classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "yfit = grid.best_estimator_.predict(xtest)\n",
    "print(classification_report(ytest, yfit, target_names=glass.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to see the misclassification we use the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "m = confusion_matrix(ytest, yfit)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(m.T, square=True, annot=True, fmt='d', cmap='Blues',\n",
    "                 xticklabels=glass.target_names, yticklabels=glass.target_names)\n",
    "ax.set(xlabel='true label', ylabel='predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we learned one of the most important details about neural networks:\n",
    "since weights are updated based on classification errors between expected and predicted\n",
    "classes, if we have classes with very little support the network will fail at classifying them.\n",
    "\n",
    "As we get collect more data the effect of classes with smaller support diminishes.\n",
    "Neural Nets work in similar fashion to us, humans, you cannot show a person 2 paintings\n",
    "by Titian and then 200 painting not by Titian and expect her to be an expert at identifying\n",
    "Titian's work.\n",
    "ANNs work best the more data you have and the more diverse the data is.\n",
    "\n",
    "For the glass data we'll leave as an exercise trying out a Random Forest,\n",
    "that model is very good at digging out classes with small support\n",
    "(contrary to decision trees!).\n",
    "But for now let's understand more about ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Concepts\n",
    "\n",
    "OK, so how does that work?\n",
    "We managed to train perceptrons because we knew what the output of each perceptron should be.\n",
    "And we saw that a perceptron can only output either -1 or 1 with the `sign` function.\n",
    "\n",
    "In neural networks the perceptrons are organized in layers:\n",
    "\n",
    "- One *input layer* which has one perceptron per feature\n",
    "- At least one *hidden layer* with fully connected perceptrons\n",
    "- One output layer with one perceptron per output class (or just one for regression)\n",
    "\n",
    "Each perceptron will have its own weights, and these will be trained simultaneously\n",
    "across the entire network, by a process called *backpropagation*.\n",
    "But before we dive into backpropagation let's discuss activation functions.\n",
    "The `sign` function is only one viable activation function,\n",
    "we can make our ANN work with several others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize=(16, 14), sharey=True, sharex=True)\n",
    "ax_size = [-5, 5, -1.5, 3]\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "# sign\n",
    "x_full = np.linspace(-5, 5, 60)\n",
    "x1 = np.linspace(-5, 0, 30)\n",
    "x2 = np.linspace(0, 5, 30)\n",
    "y1 = x1*0 - 1\n",
    "y2 = x2*0 + 1\n",
    "ax[0, 0].plot(np.hstack([x1, x2]), np.hstack([y1, y2]), color='crimson')\n",
    "ax[0, 0].legend(['$sign()$'])\n",
    "ax[0, 0].axis(ax_size)\n",
    "ax[0, 0].set_title('Sign')\n",
    "# hard limit\n",
    "y1 = x1*0\n",
    "y2 = x2*0 + 1\n",
    "ax[1, 0].plot(np.hstack([x1, x2]), np.hstack([y1, y2]), color='crimson')\n",
    "ax[1, 0].legend(['$limit()$'])\n",
    "ax[1, 0].axis(ax_size)\n",
    "ax[1, 0].set_title('Hard Limit')\n",
    "# linear\n",
    "ax[0, 1].plot(x_full, x_full/3, color='crimson')\n",
    "ax[0, 1].legend(['$y = ax + b$'])\n",
    "ax[0, 1].axis(ax_size)\n",
    "ax[0, 1].set_title('Linear')\n",
    "# relu\n",
    "y1 = x1*0\n",
    "y2 = x2\n",
    "ax[1, 1].plot(np.hstack([x1, x2]), np.hstack([y1, y2/3]), color='crimson')\n",
    "ax[1, 1].legend(['$y = 0 | y = ax$'])\n",
    "ax[1, 1].axis(ax_size)\n",
    "ax[1, 1].set_title('Rectified Linear Unit (ReLU)')\n",
    "# tanh sigmoid\n",
    "y_full = (np.exp(x_full) - np.exp(-x_full))/(np.exp(x_full) + np.exp(-x_full))\n",
    "ax[2, 0].plot(x_full, y_full, color='crimson')\n",
    "ax[2, 0].legend(['$y = (e^{x} - e^{-x})/(e^{x} + e^{-x})$'])\n",
    "ax[2, 0].axis(ax_size)\n",
    "ax[2, 0].set_title('Hyperbolic Tangent (tanh)')\n",
    "# log sigmoid\n",
    "y_full = 1/(1 + np.exp(-x_full))\n",
    "ax[2, 1].plot(x_full, y_full, color='crimson')\n",
    "ax[2, 1].legend(['$y = 1/(1 + e^{-x})$'])\n",
    "ax[2, 1].axis(ax_size)\n",
    "ax[2, 1].set_title('Log-Sigmoid (Logit)')\n",
    "# radial basis\n",
    "y_full = np.exp(- x_full*x_full)\n",
    "ax[3, 0].plot(x_full, y_full, color='crimson')\n",
    "ax[3, 0].legend(['$y = e^{-x^2}$'])\n",
    "ax[3, 0].axis(ax_size)\n",
    "ax[3, 0].set_title('Radial Basis (RBF)')\n",
    "# triangular basis\n",
    "x1 = np.linspace(-5, -1, 30)\n",
    "x2 = np.linspace(-1, 0, 10)\n",
    "x3 = np.linspace(0, 1, 10)\n",
    "x4 = np.linspace(1, 5, 30)\n",
    "y1 = x1*0\n",
    "y2 = x2 + 1\n",
    "y3 = 1 - x3\n",
    "y4 = x4*0\n",
    "ax[3, 1].plot(np.hstack([x1, x2, x3, x4]), np.hstack([y1, y2, y3, y4]), color='crimson')\n",
    "ax[3, 1].legend(['$y = ax | y = -ax$'])\n",
    "ax[3, 1].axis(ax_size)\n",
    "ax[3, 1].set_title('Triangular Basis')\n",
    "# satlin sign\n",
    "x1 = np.linspace(-5, -1, 30)\n",
    "x2 = np.linspace(-1, 0, 10)\n",
    "x3 = np.linspace(0, 1, 10)\n",
    "x4 = np.linspace(1, 5, 30)\n",
    "y1 = x1*0 - 1\n",
    "y2 = x2\n",
    "y3 = x3\n",
    "y4 = x4*0 + 1\n",
    "ax[4, 0].plot(np.hstack([x1, x2, x3, x4]), np.hstack([y1, y2, y3, y4]), color='crimson')\n",
    "ax[4, 0].legend(['$y = -1 | y = 1 | y = ax$'])\n",
    "ax[4, 0].axis(ax_size)\n",
    "ax[4, 0].set_title('Satlin Sign')\n",
    "# satlin\n",
    "x1 = np.linspace(-5, -1, 30)\n",
    "x2 = np.linspace(-1, 0, 10)\n",
    "x3 = np.linspace(0, 1, 10)\n",
    "x4 = np.linspace(1, 5, 30)\n",
    "y1 = x1*0\n",
    "y2 = x2*0\n",
    "y3 = x3\n",
    "y4 = x4*0 + 1\n",
    "ax[4, 1].plot(np.hstack([x1, x2, x3, x4]), np.hstack([y1, y2, y3, y4]), color='crimson')\n",
    "ax[4, 1].legend(['$y = 0 | y = 1 | y = ax$'])\n",
    "ax[4, 1].axis(ax_size)\n",
    "ax[4, 1].set_title('Satlin');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's not all, for the last layer of the network two other functions are often used.\n",
    "The `compet` function, which outputs a unit value for the maximum input and zeros for\n",
    "all other inputs.\n",
    "\n",
    "$$\\text{compet} = floor \\left( \\frac{x_i}{max(x)} \\right), \\text{for i in }1, 2, \\dots, N$$\n",
    "\n",
    "And the `softmax` functions which scales all inputs so that the sum of all inputs equals a unit.\n",
    "\n",
    "$$\\text{softmax} = \\frac{e^{x_i}}{\\sum_{k = 0}^{N} e^{x_k}} , \\text{for i in }1, 2, \\dots, N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Artificial Neural Network](ann-full.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ann-full.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "How can we train the perceptrons in the hidden layer?\n",
    "We do not really know what their output should be.\n",
    "We can certainly say that we have all weights in a vector, let's call is $w$,\n",
    "and then we can attempt Gradient Descent to find the best matrix.\n",
    "But GD requires us to have some form of cost (error) function since it is defined as:\n",
    "\n",
    "$$\n",
    "\\nabla E = \\frac{\\partial E}{\\partial p_1}\\hat{\\imath} + \\frac{\\partial E}{\\partial p_2}\\hat{\\jmath} + \\ldots\n",
    "$$\n",
    "\n",
    "The cost function will be multidimensional, exactly one dimension per weight\n",
    "in the vector $w$ and the last dimension being the value of the error itself.\n",
    "The total error of the network will be a sum of the error contribution of each weight\n",
    "(let's say that the vector $e$ is the amount of error each weight introduces):\n",
    "\n",
    "$$E = \\sum a(w^T \\cdot e)$$\n",
    "\n",
    "Where $a$ is the activation function.\n",
    "\n",
    "If we know the contribution of error from each weight, we can derivate the function\n",
    "above and we have all the components of the error vector to perform gradient descent.\n",
    "\n",
    "Simplifying the problem:\n",
    "If we know the classification error of one output perceptron and we know its weights,\n",
    "we can divide it across the weights coming into the perceptron, and then multiply\n",
    "by the weights to get each weight contribution.\n",
    "The contribution of each weight coming into the output perceptron is the contribution\n",
    "of the output of the perceptron in the previous layer.\n",
    "Summing up the output contributions in the previous layer\n",
    "we know the error contributions on the output side of each perceprtron on that layer.\n",
    "We can then repeat the procedure for this layer,\n",
    "and also get the contributions of the output of the previous layer.\n",
    "And so on.\n",
    "\n",
    "In the end we have the error contribution of every weight, which, when applied the\n",
    "derivate of the activation function, is one of the components of the Gradient\n",
    "vector of the cost function at the current location.\n",
    "And going in the opposite direction of the gradient vector in each of its components\n",
    "(i.e. in the direction of each weight, ignoring the dimension of the error itself)\n",
    "we are minimizing the error of the function, and therefore making the network classify better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network Execution](ann-matrix-form.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ann-matrix-form.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faces\n",
    "\n",
    "The previous more-or-less real problem we faced was face identification with the LFW dataset.\n",
    "This is a quite complex dataset but has a good number of instances.\n",
    "A neural network has a chance of performing a good-enough classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person=50)\n",
    "faces.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save computations - and time - we will perform PCA just once, on the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=200)\n",
    "faces_pca = pca.fit_transform(faces.data)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(faces_pca, faces.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are tuned on the training set only.\n",
    "This is the most important concept in ML, therefore I'll keep repeating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLPClassifier(hidden_layer_sizes=(300,), alpha=0.001, tol=0.01,\n",
    "                    max_iter=100, solver='sgd', learning_rate='constant')\n",
    "param_dict = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'tol': [0.001, 0.01],\n",
    "}\n",
    "grid = GridSearchCV(net, param_dict, cv=5)\n",
    "grid.fit(xtrain, ytrain)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reasonable network, with 300 neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the score is reasonable.\n",
    "But the full report is what we should look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = grid.predict(xtest)\n",
    "print(classification_report(ytest, yfit, target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the confusion matrix to understand misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = confusion_matrix(ytest, yfit)\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "ax = sns.heatmap(m.T, square=True, annot=True, fmt='d', cmap='BuGn',\n",
    "                 xticklabels=faces.target_names, yticklabels=faces.target_names)\n",
    "ax.set(xlabel='true label', ylabel='predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN performs reasonably and we did not even attempt to tune half\n",
    "of all its hyperparameters.\n",
    "That said, the number of hyperparameters is at the same time a strength\n",
    "and a weakness of ANNs.\n",
    "SGD training uses a lot of randomness, and some hyperparameter tuning\n",
    "may react badly to randomness.\n",
    "\n",
    "Let's try to train networks of different sizes,\n",
    "i.e. using the number of neurons in the hidden layer as a tunable hyperparameter.\n",
    "We will train each size of the network twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [10, 30, 60, 100, 200, 300, 600, 900]\n",
    "scores1 = []\n",
    "scores2 = []\n",
    "for size in sizes:\n",
    "    for s in [scores1, scores2]:\n",
    "        print('Trianing net of size', size)\n",
    "        net = MLPClassifier(hidden_layer_sizes=(size,),\n",
    "                            alpha=0.001, max_iter=100, solver='sgd')\n",
    "        net.fit(xtrain, ytrain)\n",
    "        score = net.score(xtest, ytest)\n",
    "        s.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look how the scores of all these different networks look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "ax.plot(sizes, scores1)\n",
    "ax.plot(sizes, scores2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a good deal of randomness in ANN training but we can say that around 300\n",
    "neurons (perceptrons) in the hidden layer we reached some form of plateau in accuracy score.\n",
    "The number of neurons in a layer can be used as a hyperparameter alright,\n",
    "and should be tuned with a grid search.\n",
    "Different layers in the network may be tuned in size independently,\n",
    "the computational effort to train a network can be very big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[UCI - Glass Identification Dataset][1]\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/glass+identification \"UCI Glass\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

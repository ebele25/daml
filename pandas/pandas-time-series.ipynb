{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 03 - Time Series\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>\n",
    "\n",
    "Working with data we find ourselves defining dimensions over which we want to analyze it\n",
    "(aggregate it).  Dimensions are notably known in data warehousing and analytic queries\n",
    "over such warehouses.  One such dimension that always appear for data analysis is the\n",
    "time dimension.  Windowing, changing granularity or aggregating over specific times in the\n",
    "time dimension is called time series analysis.\n",
    "\n",
    "Time series analysis requires us to be able to change the time dimension quickly, and tailor\n",
    "it to our current needs with little computation.  `pandas` provides the tools for this\n",
    "through its time indexes: time stamps, time periods and time deltas.  Let's see how we build these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "mpl.rcParams['figure.figsize'] = (12.5, 6.0)\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has the `datetime` object built into the standard library but it is quite limited.\n",
    "There is also the [dateutil][] module, by Gustavo Niemeyer, which has a much better\n",
    "date parser; and the [pytz][], by Stuart Bishop, which allows to localize times and dates\n",
    "within and between timezones.  `pandas` makes use of all three of these modules to build its\n",
    "`Timestamp`, `Period` and `Timedelta` objects.\n",
    "\n",
    "[dateutil]: https://dateutil.readthedocs.io/en/stable/ \"dateutil documentation\"\n",
    "[pytz]: http://pythonhosted.org/pytz/ \"pytz documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsed with python's dateutil\n",
    "pd.to_datetime('January, 2017'), pd.to_datetime('3rd of February 2016'), pd.to_datetime('6:31PM, Nov 11th, 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = pd.to_datetime('3rd of January 2018')\n",
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date + pd.to_timedelta(np.arange(12), 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes on dates\n",
    "\n",
    "We distinguish between three time definitions:\n",
    "the **timestamp**, e.g. at which time the plane did land;\n",
    "the **time period**, e.g. how many planes did land this Wednesday;\n",
    "and **time deltas** (or durations), e.g. how long ago did the last plane land.\n",
    "Each of these has a `pandas` object and index type:\n",
    "\n",
    "*   The `DatetimeIndex` is composed of `Timestamp` objects and the most basic date index type.\n",
    "*   `PeriodIndex` uses `Period` objects which contain `start_time` and `end_time`\n",
    "    attributes to check whether a timestamp falls within the period.\n",
    "*   And the `TimedeltaIndex` is composed of `Timedelta` objects, which represent a duration of time.\n",
    "\n",
    "We can understand periods as aggregates of timestamps and are internally defined as a single\n",
    "timestamp (start of period) and a frequency (duration of the period).  All periods within a\n",
    "`PeriodIndex` must have the same frequency.  The frequency (or duration, or offset) in `pandas`\n",
    "can be defined in many ways, with letter codes.  The most important ones are:\n",
    "\n",
    "* `D` - day\n",
    "* `B` - day, business days only\n",
    "* `W` - week\n",
    "* `M` - month\n",
    "* `A`/`Y` - year\n",
    "* `H` - hour\n",
    "* `T`/`min` - minute\n",
    "* `S` - second\n",
    "\n",
    "And these can be combined in several ways (e.g. `BAS-APR`, year starting in April on first business day).\n",
    "It is nearly impossible to remember all combinations, do have a link to the [offset documentation][offset]\n",
    "handy.  Let's see how to create time based indexes:\n",
    "\n",
    "[offset]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases \"frequency codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(['3rd of January 2016', '2016-Jul-6', '20170708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.date_range('2017-11-29', '2017-12-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-03', periods=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-03', periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.period_range('2017-09', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fremont Bridge\n",
    "\n",
    "The [Fremont bridge][fremont] in Seattle is possibly on of the most studied bridges in the world,\n",
    "it sits between the Google office in Seattle and the Adobe offices.  It is preferred by cyclists\n",
    "due to the fact that it is a small bridge linking north Seattle and downtown (the other two bridges\n",
    "are motorway bridges).  Bicycle counters were installed on both sides of the bridge in 2012, and\n",
    "are collecting data to this day (as of time of writing).\n",
    "\n",
    "The data can be downloaded from  <http://data.seattle.gov/> ([direct link][dataset]) but we\n",
    "can use the `fremont-bridge.csv` file I have already downloaded.  The file has a `Date` column\n",
    "which we will parse as the index of our time series.  Then we will try to get an understanding\n",
    "of the data (e.g. using `describe`) and see if we can plot something interesting.\n",
    "\n",
    "[fremont]: http://www.openstreetmap.org/#map=17/47.64813/-122.34965\n",
    "[dataset]: https://data.seattle.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('daml-03-04-fremont-bridge.csv', index_col='Date', parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns = ['West', 'East']\n",
    "data['Total'] = data['West'] + data['East']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.plot(alpha=0.6)\n",
    "plt.ylabel('bicycle count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probably a better granularity for the full datatset\n",
    "weekly = data.resample('W').sum()\n",
    "weekly.plot(style=[':', '--', '-'])\n",
    "plt.ylabel('bicycle count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perhaps a rolling window can be better\n",
    "daily = data.resample('D').sum()\n",
    "daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'])\n",
    "plt.ylabel('mean daily count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and if we do it on week\n",
    "weekly = data.resample('W').sum()\n",
    "weekly.rolling(10, center=True).sum().plot(style=[':', '--', '-'])\n",
    "plt.ylabel('mean weekly count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "\n",
    "Similar to an RDBMS we can group by directly on the pandas data frame.\n",
    "On a time series *grouping by* can produce completely different time frames,\n",
    "known as dicing and slicing the frame.  In general we want to *group by* to\n",
    "first filter all rows into smaller groups and then apply the aggregation\n",
    "to each of these smaller groups.\n",
    "\n",
    "On dates and times this division is quite evident: each day is formed of hours,\n",
    "each hour of minutes and so on.  Let's first try to get a series of time stamps\n",
    "from our data and see how we can divide it into divisions such as weeks, days\n",
    "or hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series = data.index.to_series()\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date and time properties of the `Series` are on an attribute called `dt`.\n",
    "This attribute has [several properties][properties] that can be used to aggregate over.\n",
    "\n",
    "[properties]: http://pandas.pydata.org/pandas-docs/version/0.20/api.html#datetimelike-properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series.dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series.dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series.groupby(series.dt.dayofweek).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series.groupby(series.dt.year).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series2012 = series['2012']\n",
    "series2012.groupby(series2012.dt.dayofweek).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, data collection started on a Wednesday so we do not have data for the first Monday and Tuesday.\n",
    "2012 ended in a Monday 31st of December, so we can see that there is an extra Monday in there.  2016\n",
    "was a leap year so we get 24 hours (and 24 data points more).\n",
    "\n",
    "But finding the missing data (remember the `data.info` above) from the `Series` is not going to be possible.\n",
    "This is because the series uses the index which is complete.  Yet, finding the missing data from\n",
    "the actual data frame is pretty easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[(data['West'].isnull()) | (data['East'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually pretty random.  Device malfunction perhaps?  I'll argue that we can safely ignore\n",
    "these missing points and go back to our full datatset.\n",
    "\n",
    "Knowing about grouping by we can slice the data in more ways now.  One thing to note is that several\n",
    "of the properties of the `Series.dt` object are directly available from a `DatetimeIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hourly traffic\n",
    "by_time = data.groupby(data.index.time).mean()\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weekly traffic\n",
    "by_weekday = data.groupby(data.index.dayofweek).mean()\n",
    "by_weekday.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "by_weekday.plot(style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# is it different on the weekend?\n",
    "weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')\n",
    "by_time = data.groupby([weekend, data.index.time]).mean()\n",
    "by_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(14, 9))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "by_time.loc['Weekday'].plot(ax=ax[0], title='Weekdays',\n",
    "                            xticks=hourly_ticks, style=[':', '--', '-'])\n",
    "by_time.loc['Weekend'].plot(ax=ax[1], title='Weekends',\n",
    "                            xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Extras\n",
    "\n",
    "* [Python Data Science Handbook - Chapter 3: Pandas - Working with time series - Jake VanderPlas][1]\n",
    "* [Is Seattle Really Seeing an Uptick In Cycling? - Jake VanderPlas][2]\n",
    "\n",
    "[1]: https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html\n",
    "[2]: https://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

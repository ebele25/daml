{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.01 Online Learning\n",
    "\n",
    "There are different models that support online learning but the technique to change\n",
    "the model parameters during the learning is (almost) always some variant on top of\n",
    "Gradient Descent (GD).\n",
    "The GD is a technique which attempts to find\n",
    "a minimal model error by walking through the function of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spruce-Fir Forest Cover](ol-forest-spruce-fir.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-spruce-fir.svg</sup></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Imagine a model with two parameters (e.g. a linear regression in 2 dimensions),\n",
    "for each combination of parameters we have some model error (misclassification\n",
    "or distance to regression line).\n",
    "The shape of this function is determined by the data to which we are fitting the model.\n",
    "\n",
    "In offline/single batch learning we fit a model by repeating techniques that find an almost\n",
    "optimal value for the parameters based on a training set.\n",
    "We use cross-validation and a test set to achieve some degree of generalization.\n",
    "If we try to fit this same model to a different set of data we would repeat the entire\n",
    "technique and end at a reasonable solution for the new dataset.\n",
    "\n",
    "With online learning we try to optimize the model parameters in a slightly different way.\n",
    "We initialize the parameters at random and calculate the gradient over this model error\n",
    "function.\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla E = \\frac{\\partial E}{\\partial w_1}\\hat{\\imath} + \\frac{\\partial E}{\\partial w_2}\\hat{\\jmath} + \\ldots\n",
    "$$\n",
    "\n",
    "i.e. it is the partial derivative against each function parameter, in two dimensions\n",
    "it is against two parameters only ($\\hat{\\imath}$ and $\\hat{\\jmath}$ are *versors*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lodgepole Pine Forest Cover](ol-forest-lodgepole-pine.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-lodgepole-pine.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient tells us how a function varies around the current model parameters,\n",
    "and therefore it tells us in which direction lie a better (lower model error) parameters.\n",
    "Yet, it does not tell us *how far away these parameter lie*.\n",
    "The *learning rate*, in online learning, is the distance that we will move in the direction\n",
    "of lower model error.\n",
    "And after we move to those parameters we will look at the gradient\n",
    "again and repeat the procedure, until convergence (or maximum iterations) is reached.\n",
    "\n",
    "By default `sklearn` uses a learning rate (`eta0`) that is reduced at each iteration,\n",
    "this allows for a technique called *Stochastic Gradient Descent* (SDG).\n",
    "In SDG only some of the samples are used at each time to determine the gradient.\n",
    "The decreasing learning rate allows for convergence despite the fact that not all samples\n",
    "are used to calculate the gradient.\n",
    "The default learning rate starts at $1/\\alpha$,\n",
    "where $\\alpha$ is the constant multiplying the regularization term.\n",
    "\n",
    "Let's try to visualize this on a surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 10, 100)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = 3*np.sin(xx) - 1 + np.sin(xx + 6) + np.cos(yy) + np.cos(yy - 0.5) + 0.6*x\n",
    "z = 1 - z/20 - 0.5\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, z, cmap='spring')\n",
    "ax.set_zlim(0, 1)\n",
    "ax.set_zlabel('Model Error', fontsize=20, labelpad=10)\n",
    "ax.set_xlabel('Model Parameter', fontsize=20, labelpad=15)\n",
    "ax.set_ylabel('Model Parameter', fontsize=20, labelpad=15)\n",
    "ax.view_init(elev=30., azim=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real problem the surface will be a high dimensional hyperplane,\n",
    "since most machine learning models work in very high dimensions.\n",
    "Yet, the same technique works on any number of dimensions.\n",
    "\n",
    "The function we try to optimize is called a *cost function*,\n",
    "and in several cases a cost function may be different from the\n",
    "actual function we are fitting the model with.\n",
    "We do not really care about all components of the gradient vector.\n",
    "Each parameter in the model is a dimension of the cost function,\n",
    "and the component of the gradient vector in that same dimension is an indicator of how\n",
    "the error changes if we change this specific parameter/weight.\n",
    "In other words the ratio of change (derivative) between error and parameter\n",
    "tells us in which direction a specific parameter should be updated to reach a smaller error.\n",
    "It is often written that instead of the gradient, what is used are\n",
    "*directed derivatives* in the direction of every parameter/weight.\n",
    "\n",
    "$$\n",
    "\\nabla E \\times \\vec{u} = \\| \\nabla E \\|_2 \\| u \\|_2 \\cos \\theta\n",
    "$$\n",
    "\n",
    "For example the `Ridge` regression added an extra term to the function\n",
    "we tried to optimize, and that extra term allowed us for a better fit.\n",
    "The gradient descent optimizes the cost function but the model itself\n",
    "predicts values based on the actual fitted function (from the model\n",
    "parameters only).\n",
    "In other words, we have the function (e.g. classification) we are trying to optimize,\n",
    "and to find that we build and optimize a cost function,\n",
    "which is a completely different function.\n",
    "\n",
    "In `sklearn` we have `SDGClassifier` which will perform the technique\n",
    "above to achieve online learning on top of linear SVMs, logistic regression, or a perceptron.\n",
    "The `SDGRegressor` performs a linear regression as online learning.\n",
    "Note that this means that we can only find solutions to problems\n",
    "that can be approximated linearly.\n",
    "For non-linear online learning we need neural networks (which we will see soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Direction](ol-gradient-direction.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-gradient-direction.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest Cover Type Dataset\n",
    "\n",
    "For a change let's take on a dataset that is not present inside `sklearn`.\n",
    "The forest cover dataset are cartographic data about types of forests\n",
    "in the Roosevelt National Forest in Colorado.\n",
    "First let's define a couple of details about the features of the dataset.\n",
    "It has several continuous features and then several categorical ones.\n",
    "The categorical features are already one-hot-encoded for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = [\n",
    "    'Elevation',\n",
    "    'Aspect',\n",
    "    'Slope',\n",
    "    'HHydro',  # Horizontal Distance to Hydrology\n",
    "    'VHydro',  # Vertical Distance to Hydrology\n",
    "    'Road',    # Horizontal Distance to Roadways\n",
    "    'Shade_9am',\n",
    "    'Shade_Noon',\n",
    "    'Shade_3pm',\n",
    "    'Fire',    # Horizontal Distance to Fire Points\n",
    "]\n",
    "categorical = [\n",
    "    'wild=1',  # Rawah Wilderness Area\n",
    "    'wild=2',  # Neota Wilderness Area\n",
    "    'wild=3',  # Comanche Peak Wilderness Area\n",
    "    'wild=4',  # Cache la Poudre Wilderness Area\n",
    "    'soil=1','soil=2','soil=3','soil=4','soil=5','soil=6','soil=7','soil=8','soil=9','soil=10',\n",
    "    'soil=11','soil=12','soil=13','soil=14','soil=15','soil=16','soil=17','soil=18','soil=19','soil=20',\n",
    "    'soil=21','soil=22','soil=23','soil=24','soil=25','soil=26','soil=27','soil=28','soil=29','soil=30',\n",
    "    'soil=31','soil=32','soil=33','soil=34','soil=35','soil=36','soil=37','soil=38','soil=39','soil=40',\n",
    "]\n",
    "columns = continuous + categorical + ['label']\n",
    "target_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine',\n",
    "                'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ponderosa Pine Forest Cover](ol-forest-ponderosa-pine.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-ponderosa-pine.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the features we can then classify an area of forest cover into\n",
    "one of the seven classification (targets/labels).\n",
    "The set has more then half a million rows of data,\n",
    "it a reasonably sized dataset.\n",
    "\n",
    "To keep with the spirit of what we have been doing until now we will\n",
    "write a function to actually retrieve the dataset.\n",
    "We will duplicate the `sklearn` convention for dataset loading and construct\n",
    "our `load_cov_type` function.\n",
    "The function not only allows for easy download of the dataset\n",
    "but also caches the downloaded data on the filesystem,\n",
    "so one does not need to download it the next time.\n",
    "A couple of things worth mentioning are that:\n",
    "the dataset is taken from the\n",
    "*University of California Irvine Machine Learning Repository*\n",
    "and is kept within their archives g-zipped.\n",
    "Also, the labels in the dataset start from $1$,\n",
    "we adjust the labels to start from $0$ during the dataset load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zlib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "def load_cover_type():\n",
    "    cov_dir = 'uci_cover_type'\n",
    "    data_dir = datasets.get_data_home()\n",
    "    data_path = os.path.join(data_dir, cov_dir, 'covtype.data')\n",
    "    descr_path = os.path.join(data_dir, cov_dir, 'covtype.info')\n",
    "    cov_data_gz = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "    cov_descr = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info'\n",
    "    os.makedirs(os.path.join(data_dir, cov_dir), exist_ok=True)\n",
    "    try:\n",
    "        with open(descr_path, 'r') as f:\n",
    "            descr = f.read()\n",
    "    except IOError:\n",
    "        print('Downloading file from', cov_descr, file=sys.stderr)\n",
    "        r = requests.get(cov_descr)\n",
    "        with open(descr_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        descr = r.text\n",
    "        r.close()\n",
    "    try:\n",
    "        data = pd.read_csv(data_path, delimiter=',', names=columns)\n",
    "    except IOError:\n",
    "        print('Downloading file from', cov_data_gz, file=sys.stderr)\n",
    "        r = requests.get(cov_data_gz)\n",
    "        cov_data = zlib.decompress(r.content, wbits=16+zlib.MAX_WBITS)  # obscure but works\n",
    "        cov_data = cov_data.decode('utf8')\n",
    "        with open(data_path, 'w') as f:\n",
    "            f.write(cov_data)\n",
    "        r.close()\n",
    "        data = pd.read_csv(data_path, delimiter=',', names=columns)\n",
    "    X = data[continuous + categorical].values\n",
    "    y = data['label'].values - 1\n",
    "    return Bunch(DESCR=descr,\n",
    "                 data=X,\n",
    "                 feature_names=columns[:-1],\n",
    "                 feature_continuous=continuous,\n",
    "                 feature_categorical=categorical,\n",
    "                 target=y,\n",
    "                 target_names=target_names)\n",
    "\n",
    "\n",
    "covtype = load_cover_type()\n",
    "print(covtype.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the dataset is always a good idea.\n",
    "One can see all three parts of the set:\n",
    "the continuous features, the one-hot-encoded categorical features,\n",
    "and the forest cover type labels.\n",
    "\n",
    "In the loading function we have also added a distinction between\n",
    "continuous and categorical features.\n",
    "This concept is often useful as one may need to scale continuous\n",
    "features but scaling one-hot-encoded features makes little sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('covtype.data', delimiter=',', names=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cottonwood Forest Cover](ol-forest-cottonwood.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-cottonwood.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half a million rows is a good dataset.\n",
    "It perhaps does not require online learning on most machines\n",
    "but on some it might.\n",
    "\n",
    "That said, for presentation purposes it may take too long\n",
    "to run out code on the full dataset.\n",
    "Instead we will take two forest types: Ponderosa Pine and Douglas-fir,\n",
    "and use only the part of the dataset with these labels.\n",
    "Note how we change the labels to be $0$ and $1$\n",
    "for a classification between only two forest types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = covtype.data\n",
    "y = covtype.target\n",
    "X = X[(y == 2) | (y == 5)]\n",
    "y = y[(y == 2) | (y == 5)]\n",
    "y[y == 2] = 0  # Ponderosa Pine\n",
    "y[y == 5] = 1  # Douglas-fir\n",
    "df = pd.DataFrame(X, columns=covtype.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Willow Forest Cover](ol-forest-willow.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-willow.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we can easily see that the continuous features\n",
    "have very distinct value ranges.\n",
    "And therefore will require scaling.\n",
    "\n",
    "We have the columns that are continuous in an attribute\n",
    "of the loaded dataset.\n",
    "If we now scale only those columns and place them back together\n",
    "with the categorical columns we have a dataset we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_cont = sc.fit_transform(df[covtype.feature_continuous].values)\n",
    "X_cat = df[covtype.feature_categorical].values\n",
    "X = np.c_[X_cont, X_cat]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a real dataset, we should treat it as a real problem.\n",
    "We take out a test set which we will not touch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train on the training set with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = make_pipeline(\n",
    "    PCA(n_components=20),\n",
    "    SGDClassifier(loss='log', penalty='l1', max_iter=500, alpha=0.01, tol=0.01))\n",
    "param_grid = {\n",
    "    'sgdclassifier__alpha': [0.001, 0.01, 0.1],\n",
    "    'sgdclassifier__tol': [0.01, 0.1],\n",
    "}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(xtrain, ytrain)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Aspen Forest Cover](ol-forest-aspen.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-aspen.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the models during grid search may not converge.\n",
    "That's completely fine, we did root them out through the cross-validation.\n",
    "The best model is likely to be one that did converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "names = ['Ponderosa Pine', 'Douglas-fir']\n",
    "yfit = grid.best_estimator_.predict(xtest)\n",
    "print(classification_report(ytest, yfit, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Douglas-Fir Forest Cover](ol-forest-douglas-fir.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-douglas-fir.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey!  That wasn't online learning.\n",
    "\n",
    "Right, it was not.\n",
    "The full dataset may need online learning\n",
    "but out sample of two forest types only fit fine in memory.\n",
    "Yet we can make up the idea of a dataset that is too big\n",
    "by slitting into two sets of data and train SGD in an online fashion.\n",
    "We will *misuse* the `train_test_split` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov1, cov2, ycov1, ycov2 = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn` there are two ways of using online learning.\n",
    "One is to use a method called `partial_fit`, instead of `fit`,\n",
    "which will update parameters instead of fitting completely new ones.\n",
    "Another way to enable online learning is to pass `warm_start=True`,\n",
    "this forces `fit` to always work like `partial_fit`.\n",
    "\n",
    "Both methods only work on models that inherently support online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1, xtest1, ytrain1, ytest1 = train_test_split(cov1, ycov1, test_size=0.2)\n",
    "model = make_pipeline(\n",
    "    PCA(n_components=10),\n",
    "    SGDClassifier(loss='hinge', penalty='l1', max_iter=200, alpha=0.001, tol=0.01, warm_start=True))\n",
    "model.fit(xtrain1, ytrain1)\n",
    "yfit = model.predict(xtest1)\n",
    "print(classification_report(ytest1, yfit, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know about half of the data and we can, more-or-less, classify that.\n",
    "But if we try to classify the data we do not know about we may run into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain2, xtest2, ytrain2, ytest2 = train_test_split(cov2, ycov2, test_size=0.2)\n",
    "yfit = model.predict(xtest2)\n",
    "print(classification_report(ytest2, yfit, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train with some data from the second dataset and see if things improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain2, ytrain2)\n",
    "yfit = model.predict(xtest2)\n",
    "print(classification_report(ytest2, yfit, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Krummholz Forest Cover](ol-forest-krummholz.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-forest-krummholz.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Non-GD Optimisation\n",
    "\n",
    "We saw SGD and said that it is the most often used optimization technique.\n",
    "But what are the others?\n",
    "One technique is **simulated annealing** which works by slow cooling.\n",
    "In summary: simulated annealing tries random neighbors at each iteration and keeps\n",
    "track of of the point with the lowest value of the cost function.\n",
    "The search space for a new neighbor (i.e. the maximum distance form the lowest point\n",
    "found until now) reduces at each iteration.\n",
    "This is similar to SGD with a decreasing learning rate.\n",
    "\n",
    "But there are more techniques.\n",
    "Notably **swarm intelligence** provides us with several optimization algorithms:\n",
    "\n",
    "- particle swarm\n",
    "- bat swarm\n",
    "- cuckoo search\n",
    "\n",
    "And **genetic algorithms** also work reasonably in an online learning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[UCI - Forest Cover Type Dataset][1]\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/Covertype \"UCI Forest Cover Type\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
